{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WGAN",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1_Y75QXJS6h"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZKbyU2-AiY-",
        "outputId": "73396c2e-3bd2-4e31-8f59-656f47e15cb8"
      },
      "source": [
        "#based on Keras Implementation\n",
        "!pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.33.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.5)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.35.1)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow) (50.3.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.3.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.11.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (2.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKq-Gc1_EyJk"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx-zNbLqB4K8"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time\n",
        "import imageio\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Dropout\n",
        "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzTlj4YdCip_",
        "outputId": "64d1cee0-99c9-41d8-97ec-41e77e9cb890"
      },
      "source": [
        "# To generate GIFs\n",
        "!pip install gitpython\n",
        "!pip install -q imageio\n",
        "!pip install -q git+https://github.com/tensorflow/docs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gitpython\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/d1/a7f8fe3df258549b303415157328bfcc63e9b11d06a7ad7a3327f3d32606/GitPython-3.1.11-py3-none-any.whl (159kB)\n",
            "\r\u001b[K     |██                              | 10kB 28.5MB/s eta 0:00:01\r\u001b[K     |████▏                           | 20kB 31.0MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 30kB 23.6MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 40kB 20.4MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 51kB 17.7MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 61kB 16.2MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 71kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 81kB 14.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 92kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 102kB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 112kB 15.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 122kB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 133kB 15.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 143kB 15.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 153kB 15.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 163kB 15.5MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\r\u001b[K     |█████▏                          | 10kB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20kB 32.9MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 30kB 38.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 40kB 29.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 51kB 32.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 61kB 28.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 10.1MB/s \n",
            "\u001b[?25hCollecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
            "Installing collected packages: smmap, gitdb, gitpython\n",
            "Successfully installed gitdb-4.0.5 gitpython-3.1.11 smmap-3.0.4\n",
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfIk2es3hJEd",
        "outputId": "210288cd-0b2a-40eb-fdec-52a1b81aef27"
      },
      "source": [
        "!pip install matplotlib\n",
        "!pip install numpy\n",
        "\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "from IPython import display\n",
        "import time\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.18.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TszVMPeHEyJm"
      },
      "source": [
        "import os\n",
        "import tqdm.notebook as tq\n",
        "from os import listdir\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "import matplotlib.image as mpimg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iP_oD9aiEyJm",
        "outputId": "3d1d4bbc-23f4-4848-cb1d-0b8e6a236aa8"
      },
      "source": [
        "import sys\n",
        "from PIL import Image\n",
        "sys.modules['Image'] = Image \n",
        "\n",
        "from PIL import Image\n",
        "print(Image.__file__)\n",
        "\n",
        "import Image\n",
        "print(Image.__file__)\n",
        "\n",
        "import PIL\n",
        "\n",
        "!pip install sklearn\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.17.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYn4MdZnKCey"
      },
      "source": [
        "### Load and prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpFaCcRwEyJm"
      },
      "source": [
        "from tensorflow.keras.layers import (Dense, \n",
        "                                     BatchNormalization, \n",
        "                                     LeakyReLU, \n",
        "                                     Reshape, \n",
        "                                     Conv2DTranspose,\n",
        "                                     Conv2D,\n",
        "                                     Dropout,\n",
        "                                     Flatten)\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDLry8UqEyJm"
      },
      "source": [
        "#--some helpful stuff for loading images\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxMzTvCBEyJm"
      },
      "source": [
        "from PIL import Image\n",
        "from PIL.ImageOps import grayscale\n",
        "#--loading images\n",
        "#trainfolder='/content/drive/MyDrive/20F/ENGS108_Final_Project/Data_Sets/maps/maps/trainA/'\n",
        "# Andra's:\n",
        "#trainfolder='/content/drive/MyDrive/ENGS 108/ENGS108_Final_Project/Data_Sets/maps/maps/trainB/'\n",
        "trainfolder = '/Users/Andrada Pantelimon/Google Drive/ENGS 108/project/data_sets/trainB/'\n",
        "# Vlado's\n",
        "#trainfolder='/content/drive/MyDrive/ENGS108_Final_Project/Data_Sets/maps/maps/trainB/'\n",
        "trainfolder='/content/drive/MyDrive/ENGS108_Final_Project/Data_Sets/roads_only/'\n",
        "#trainfolder='/content/drive/MyDrive/ENGS108_Final_Project/Data_Sets/maps/maps/trainB/'\n",
        "\n",
        "\n",
        "\n",
        "size=(256,256)\n",
        "\n",
        "trainimages=list()\n",
        "for filename in listdir(trainfolder):\n",
        "      # load and resize the image\n",
        "      pixels = load_img(trainfolder + filename, target_size=size)\n",
        "      pixels = grayscale(pixels)\n",
        "      # convert to numpy array\n",
        "      pixels = img_to_array(pixels)\n",
        "      # Normalize the images to [-1, 1]\n",
        "      pixels=(pixels- 127.5) / 127.5\n",
        "      # store\n",
        "      trainimages.append(pixels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "f4yfeTmIEyJn",
        "outputId": "3b05b911-fa47-4404-8df5-97b8d328f77e"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "#--plot images\n",
        "n_samples = 3\n",
        "for i in range(n_samples):\n",
        "    pyplot.subplot(2, n_samples, 1 + i)\n",
        "    pyplot.axis('off')\n",
        "    print(min(trainimages[i][0]))\n",
        "    pyplot.imshow(trainimages[i][:,:,0].astype('uint8'),cmap='gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.]\n",
            "[1.]\n",
            "[1.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4PIDhoDLbsZ"
      },
      "source": [
        "np_train = np.array(trainimages)\n",
        "BATCH_SIZE = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHWirlo7FZP7",
        "outputId": "d8de9e5a-42d4-47ab-e7f0-048d941214d0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yKCCQOoJ7cn"
      },
      "source": [
        "# Batch and shuffle the data\n",
        "#train_dataset = tf.data.Dataset.from_tensor_slices(trainimages).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3XpAVKYEyJn"
      },
      "source": [
        "#train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAMPmKgNEyJn"
      },
      "source": [
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THY-sZMiQ4UV"
      },
      "source": [
        "## Create the models\n",
        "\n",
        "Both the generator and discriminator are defined using the [Keras Sequential API](https://www.tensorflow.org/guide/keras#sequential_model)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tEyxE-GMC48"
      },
      "source": [
        "### The Generator\n",
        "\n",
        "The generator uses `tf.keras.layers.Conv2DTranspose` (upsampling) layers to produce an image from a seed (random noise). Start with a `Dense` layer that takes this seed as input, then upsample several times until you reach the desired image size of 28x28x1. Notice the `tf.keras.layers.LeakyReLU` activation for each layer, except the output layer which uses tanh."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyWgG09LCSJl"
      },
      "source": [
        "Use the (as yet untrained) generator to create an image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0IKnaCtg6WE"
      },
      "source": [
        "### The Discriminator\n",
        "\n",
        "The discriminator is a CNN-based image classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw2tPLmk2pEP"
      },
      "source": [
        "IMG_SHAPE = (256,256,1)\n",
        "def conv_block(\n",
        "    x,\n",
        "    filters,\n",
        "    activation,\n",
        "    kernel_size=(3, 3),\n",
        "    strides=(1, 1),\n",
        "    padding=\"same\",\n",
        "    use_bias=True,\n",
        "    use_bn=False,\n",
        "    use_dropout=False,\n",
        "    drop_value=0.5,\n",
        "):\n",
        "    x = layers.Conv2D(\n",
        "        filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias\n",
        "    )(x)\n",
        "    if use_bn:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "    x = activation(x)\n",
        "    if use_dropout:\n",
        "        x = layers.Dropout(drop_value)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def define_discriminator():\n",
        "    img_input = layers.Input(shape=IMG_SHAPE)\n",
        "    # Zero pad the input to make the input images size to (32, 32, 1).\n",
        "   # x = layers.ZeroPadding2D((2, 2))(img_input)\n",
        "    x = conv_block(\n",
        "        img_input,\n",
        "        64,\n",
        "        kernel_size=(5, 5),\n",
        "        strides=(2, 2),\n",
        "        use_bn=False,\n",
        "        use_bias=True,\n",
        "        activation=layers.LeakyReLU(0.2),\n",
        "        use_dropout=False,\n",
        "        drop_value=0.3,\n",
        "    )\n",
        "    x = conv_block(\n",
        "        x,\n",
        "        128,\n",
        "        kernel_size=(5, 5),\n",
        "        strides=(2, 2),\n",
        "        use_bn=False,\n",
        "        activation=layers.LeakyReLU(0.2),\n",
        "        use_bias=True,\n",
        "        use_dropout=True,\n",
        "        drop_value=0.3,\n",
        "    )\n",
        "    x = conv_block(\n",
        "        x,\n",
        "        256,\n",
        "        kernel_size=(5, 5),\n",
        "        strides=(2, 2),\n",
        "        use_bn=False,\n",
        "        activation=layers.LeakyReLU(0.2),\n",
        "        use_bias=True,\n",
        "        use_dropout=True,\n",
        "        drop_value=0.3,\n",
        "    )\n",
        "    x = conv_block(\n",
        "        x,\n",
        "        512,\n",
        "        kernel_size=(5, 5),\n",
        "        strides=(2, 2),\n",
        "        use_bn=False,\n",
        "        activation=layers.LeakyReLU(0.2),\n",
        "        use_bias=True,\n",
        "        use_dropout=False,\n",
        "        drop_value=0.3,\n",
        "    )\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.Dense(1)(x)\n",
        "\n",
        "    d_model = keras.models.Model(img_input, x, name=\"discriminator\")\n",
        "    return d_model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhPneagzCaQv"
      },
      "source": [
        "Use the (as yet untrained) discriminator to classify the generated images as real or fake. The model will be trained to output positive values for real images, and negative values for fake images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDkA05NE6QMs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "903c9920-5258-4146-cec3-e027d06499c4"
      },
      "source": [
        "discriminator = define_discriminator()\n",
        "#decision = discriminator(generated_image)\n",
        "#print (decision)\n",
        "discriminator.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 256, 256, 1)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 128, 128, 64)      1664      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 128, 128, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 64, 64, 128)       204928    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 32, 32, 256)       819456    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 16, 16, 512)       3277312   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 16, 16, 512)       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 131072)            0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 131072)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 131073    \n",
            "=================================================================\n",
            "Total params: 4,434,433\n",
            "Trainable params: 4,434,433\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FMYgY_mPfTi"
      },
      "source": [
        "## Define the loss and optimizers\n",
        "\n",
        "Define loss functions and optimizers for both models.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psQfmXxYKU3X"
      },
      "source": [
        "noise_dim=100\n",
        "def upsample_block(\n",
        "    x,\n",
        "    filters,\n",
        "    activation,\n",
        "    kernel_size=(3, 3),\n",
        "    strides=(1, 1),\n",
        "    up_size=(2, 2),\n",
        "    padding=\"same\",\n",
        "    use_bn=False,\n",
        "    use_bias=True,\n",
        "    use_dropout=False,\n",
        "    drop_value=0.3,\n",
        "):\n",
        "    x = layers.UpSampling2D(up_size)(x)\n",
        "    x = layers.Conv2D(\n",
        "        filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias\n",
        "    )(x)\n",
        "\n",
        "    if use_bn:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "    if activation:\n",
        "        x = activation(x)\n",
        "    if use_dropout:\n",
        "        x = layers.Dropout(drop_value)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def define_generator():\n",
        "    noise = layers.Input(shape=(noise_dim,))\n",
        "    x = layers.Dense(4 * 4 * 256, use_bias=False)(noise)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = layers.Reshape((4, 4, 256))(x)\n",
        "    x = upsample_block(\n",
        "        x,\n",
        "        128,\n",
        "        layers.LeakyReLU(0.2),\n",
        "        strides=(1, 1),\n",
        "        use_bias=False,\n",
        "        use_bn=True,\n",
        "        padding=\"same\",\n",
        "        use_dropout=False,\n",
        "    )\n",
        "    x = upsample_block(\n",
        "        x,\n",
        "        64,\n",
        "        layers.LeakyReLU(0.2),\n",
        "        strides=(1, 1),\n",
        "        use_bias=False,\n",
        "        use_bn=True,\n",
        "        padding=\"same\",\n",
        "        use_dropout=False,\n",
        "    )\n",
        "    x = upsample_block(\n",
        "        x,\n",
        "        64,\n",
        "        layers.LeakyReLU(0.2),\n",
        "        strides=(1, 1),\n",
        "        use_bias=False,\n",
        "        use_bn=True,\n",
        "        padding=\"same\",\n",
        "        use_dropout=False,\n",
        "    )\n",
        "    x = upsample_block(\n",
        "        x,\n",
        "        64,\n",
        "        layers.LeakyReLU(0.2),\n",
        "        strides=(1, 1),\n",
        "        use_bias=False,\n",
        "        use_bn=True,\n",
        "        padding=\"same\",\n",
        "        use_dropout=False,\n",
        "    )\n",
        "    x = upsample_block(\n",
        "        x,\n",
        "        64,\n",
        "        layers.LeakyReLU(0.2),\n",
        "        strides=(1, 1),\n",
        "        use_bias=False,\n",
        "        use_bn=True,\n",
        "        padding=\"same\",\n",
        "        use_dropout=False,\n",
        "    )\n",
        "    x = upsample_block(\n",
        "        x, 1, layers.Activation(\"tanh\"), strides=(1, 1), use_bias=False, use_bn=True\n",
        "    )\n",
        "    # At this point, we have an output which has the same shape as the input, (32, 32, 1).\n",
        "    # We will use a Cropping2D layer to make it (28, 28, 1).\n",
        "  #  x = layers.Cropping2D((2, 2))(x)\n",
        "\n",
        "    g_model = keras.models.Model(noise, x, name=\"generator\")\n",
        "    return g_model\n",
        "\n",
        "\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "    # generate points in the latent space\n",
        "    x_input = randn(latent_dim * n_samples)\n",
        "    # reshape into a batch of inputs for the network\n",
        "    x_input = x_input.reshape(n_samples, latent_dim)\n",
        "    return x_input\n",
        "\n",
        "# use the generator to generate n fake examples, with class labels\n",
        "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
        "    # generate points in latent space\n",
        "    x_input = generate_latent_points(latent_dim, n_samples)\n",
        "    # predict outputs\n",
        "    X = g_model.predict(x_input)\n",
        "    # create 'fake' class labels (0)\n",
        "    y = np.zeros((n_samples,1))\n",
        "  #  y = np.random.uniform(0,0.3, n_samples)\n",
        "    return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNroCmiV1TM6",
        "outputId": "5b8ac118-8710-422e-ef68-392221c07870"
      },
      "source": [
        "generator = define_generator()\n",
        "generator.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 100)]             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4096)              409600    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d (UpSampling2D) (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 8, 8, 128)         294912    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 16, 16, 64)        73728     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 32, 32, 64)        36864     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2 (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 64, 64, 64)        36864     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 64, 64, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_4 (UpSampling2 (None, 128, 128, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 128, 128, 64)      36864     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 128, 128, 64)      256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 128, 128, 64)      0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_5 (UpSampling2 (None, 256, 256, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 256, 256, 1)       576       \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 256, 256, 1)       4         \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 256, 256, 1)       0         \n",
            "=================================================================\n",
            "Total params: 907,332\n",
            "Trainable params: 898,370\n",
            "Non-trainable params: 8,962\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKY_iPSPNWoj"
      },
      "source": [
        "### Discriminator loss\n",
        "\n",
        "This method quantifies how well the discriminator is able to distinguish real images from fakes. It compares the discriminator's predictions on real images to an array of 1s, and the discriminator's predictions on fake (generated) images to an array of 0s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkMNfBWlT-PV"
      },
      "source": [
        "import keras\n",
        "class WGAN(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        discriminator,\n",
        "        generator,\n",
        "        latent_dim,\n",
        "        discriminator_extra_steps=3,\n",
        "        gp_weight=10.0,\n",
        "    ):\n",
        "        super(WGAN, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.d_steps = discriminator_extra_steps\n",
        "        self.gp_weight = gp_weight\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
        "        super(WGAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.d_loss_fn = d_loss_fn\n",
        "        self.g_loss_fn = g_loss_fn\n",
        "\n",
        "    def gradient_penalty(self, batch_size, real_images, fake_images):\n",
        "        \"\"\" Calculates the gradient penalty.\n",
        "\n",
        "        This loss is calculated on an interpolated image\n",
        "        and added to the discriminator loss.\n",
        "        \"\"\"\n",
        "        # get the interplated image\n",
        "        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
        "        diff = fake_images - real_images\n",
        "        interpolated = real_images + alpha * diff\n",
        "\n",
        "        with tf.GradientTape() as gp_tape:\n",
        "            gp_tape.watch(interpolated)\n",
        "            # 1. Get the discriminator output for this interpolated image.\n",
        "            pred = self.discriminator(interpolated, training=True)\n",
        "\n",
        "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
        "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
        "        # 3. Calcuate the norm of the gradients\n",
        "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
        "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
        "        return gp\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        if isinstance(real_images, tuple):\n",
        "            real_images = real_images[0]\n",
        "\n",
        "        # Get the batch size\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "\n",
        "        # For each batch, we are going to perform the\n",
        "        # following steps as laid out in the original paper.\n",
        "        # 1. Train the generator and get the generator loss\n",
        "        # 2. Train the discriminator and get the discriminator loss\n",
        "        # 3. Calculate the gradient penalty\n",
        "        # 4. Multiply this gradient penalty with a constant weight factor\n",
        "        # 5. Add gradient penalty to the discriminator loss\n",
        "        # 6. Return generator and discriminator losses as a loss dictionary.\n",
        "\n",
        "        # Train discriminator first. The original paper recommends training\n",
        "        # the discriminator for `x` more steps (typically 5) as compared to\n",
        "        # one step of the generator. Here we will train it for 3 extra steps\n",
        "        # as compared to 5 to reduce the training time.\n",
        "        for i in range(self.d_steps):\n",
        "            # Get the latent vector\n",
        "            random_latent_vectors = tf.random.normal(\n",
        "                shape=(batch_size, self.latent_dim)\n",
        "            )\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Generate fake images from the latent vector\n",
        "                fake_images = self.generator(random_latent_vectors, training=True)\n",
        "                # Get the logits for the fake images\n",
        "                fake_logits = self.discriminator(fake_images, training=True)\n",
        "                # Get the logits for real images\n",
        "                real_logits = self.discriminator(real_images, training=True)\n",
        "\n",
        "                # Calculate discriminator loss using fake and real logits\n",
        "                d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n",
        "                # Calculate the gradient penalty\n",
        "                gp = self.gradient_penalty(batch_size, real_images, fake_images)\n",
        "                # Add the gradient penalty to the original discriminator loss\n",
        "                d_loss = d_cost + gp * self.gp_weight\n",
        "\n",
        "            # Get the gradients w.r.t the discriminator loss\n",
        "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
        "            # Update the weights of the discriminator using the discriminator optimizer\n",
        "            self.d_optimizer.apply_gradients(\n",
        "                zip(d_gradient, self.discriminator.trainable_variables)\n",
        "            )\n",
        "\n",
        "        # Train the generator now.\n",
        "        # Get the latent vector\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Generate fake images using the generator\n",
        "            generated_images = self.generator(random_latent_vectors, training=True)\n",
        "            # Get the discriminator logits for fake images\n",
        "            gen_img_logits = self.discriminator(generated_images, training=True)\n",
        "            # Calculate the generator loss\n",
        "            g_loss = self.g_loss_fn(gen_img_logits)\n",
        "\n",
        "        # Get the gradients w.r.t the generator loss\n",
        "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
        "        # Update the weights of the generator using the generator optimizer\n",
        "        self.g_optimizer.apply_gradients(\n",
        "            zip(gen_gradient, self.generator.trainable_variables)\n",
        "        )\n",
        "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
        "\n",
        "\n",
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=1, latent_dim=128):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images = (generated_images * 127.5) + 127.5\n",
        "\n",
        "        for i in range(self.num_img):\n",
        "            img = generated_images[i].numpy()\n",
        "            img = keras.preprocessing.image.array_to_img(img)\n",
        "            img.save(\"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWCn_PVdEJZ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc7b8bd7-628b-4197-991d-406bd4bf5e8b"
      },
      "source": [
        "# Optimizer for both the networks\n",
        "noise_dim=128\n",
        "# learning_rate=0.0002, beta_1=0.5 are recommened\n",
        "generator_optimizer = keras.optimizers.Adam(\n",
        "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
        ")\n",
        "discriminator_optimizer = keras.optimizers.Adam(\n",
        "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
        ")\n",
        "\n",
        "# Define the loss functions to be used for discrimiator\n",
        "# This should be (fake_loss - real_loss)\n",
        "# We will add the gradient penalty later to this loss function\n",
        "def discriminator_loss(real_img, fake_img):\n",
        "    real_loss = tf.reduce_mean(real_img)\n",
        "    fake_loss = tf.reduce_mean(fake_img)\n",
        "    return fake_loss - real_loss\n",
        "\n",
        "\n",
        "# Define the loss functions to be used for generator\n",
        "def generator_loss(fake_img):\n",
        "    return -tf.reduce_mean(fake_img)\n",
        "\n",
        "\n",
        "# Epochs to train\n",
        "epochs = 20\n",
        "\n",
        "# Callbacks\n",
        "cbk = GANMonitor(num_img=3, latent_dim=noise_dim)\n",
        "\n",
        "disc = define_discriminator()\n",
        "gen = define_generator()\n",
        "# Get the wgan model\n",
        "wgan = WGAN(\n",
        "    discriminator=disc,\n",
        "    generator=gen,\n",
        "    latent_dim=noise_dim,\n",
        "    discriminator_extra_steps=3,\n",
        ")\n",
        "\n",
        "# Compile the wgan model\n",
        "wgan.compile(\n",
        "    d_optimizer=discriminator_optimizer,\n",
        "    g_optimizer=generator_optimizer,\n",
        "    g_loss_fn=generator_loss,\n",
        "    d_loss_fn=discriminator_loss,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "wgan.fit(np_train, batch_size=40, epochs=10000, callbacks=[cbk])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1142.2263 - g_loss: 154.3759\n",
            "Epoch 2/10000\n",
            "9/9 [==============================] - 21s 2s/step - d_loss: -1515.4203 - g_loss: 21.3350\n",
            "Epoch 3/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1374.9853 - g_loss: -108.9541\n",
            "Epoch 4/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1291.4415 - g_loss: -457.1795\n",
            "Epoch 5/10000\n",
            "9/9 [==============================] - 23s 3s/step - d_loss: -1327.9957 - g_loss: -544.9983\n",
            "Epoch 6/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1308.5297 - g_loss: -440.3637\n",
            "Epoch 7/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1513.1161 - g_loss: -8.8810\n",
            "Epoch 8/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1596.0254 - g_loss: 259.9406\n",
            "Epoch 9/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1600.5569 - g_loss: 605.2763\n",
            "Epoch 10/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1635.5820 - g_loss: 1022.2941\n",
            "Epoch 11/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1587.5368 - g_loss: 978.4551\n",
            "Epoch 12/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1726.4766 - g_loss: 1386.6033\n",
            "Epoch 13/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1683.5908 - g_loss: 1383.7947\n",
            "Epoch 14/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1729.3905 - g_loss: 1518.8136\n",
            "Epoch 15/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1650.5710 - g_loss: 1651.8518\n",
            "Epoch 16/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1685.7187 - g_loss: 1830.7366\n",
            "Epoch 17/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1553.8884 - g_loss: 1857.0387\n",
            "Epoch 18/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1693.3105 - g_loss: 1918.0409\n",
            "Epoch 19/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1651.4428 - g_loss: 1927.8958\n",
            "Epoch 20/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1668.4219 - g_loss: 2066.8293\n",
            "Epoch 21/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1609.6725 - g_loss: 2159.6164\n",
            "Epoch 22/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1583.0284 - g_loss: 2328.6259\n",
            "Epoch 23/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1613.0853 - g_loss: 2393.4141\n",
            "Epoch 24/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1587.4209 - g_loss: 2341.6215\n",
            "Epoch 25/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1756.5349 - g_loss: 2650.0064\n",
            "Epoch 26/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1769.0860 - g_loss: 2818.3549\n",
            "Epoch 27/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1757.8721 - g_loss: 3032.6516\n",
            "Epoch 28/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1514.4787 - g_loss: 3249.8357\n",
            "Epoch 29/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1617.0867 - g_loss: 4370.5526\n",
            "Epoch 30/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1506.5460 - g_loss: 5535.5504\n",
            "Epoch 31/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1618.8798 - g_loss: 5861.8840\n",
            "Epoch 32/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1676.5976 - g_loss: 5763.1702\n",
            "Epoch 33/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1516.9941 - g_loss: 4977.7846\n",
            "Epoch 34/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1645.9907 - g_loss: 5386.9172\n",
            "Epoch 35/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1706.9233 - g_loss: 4670.2576\n",
            "Epoch 36/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1627.5616 - g_loss: 5269.5252\n",
            "Epoch 37/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1450.9854 - g_loss: 5357.2314\n",
            "Epoch 38/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1603.1114 - g_loss: 5848.1172\n",
            "Epoch 39/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1583.3536 - g_loss: 6213.5352\n",
            "Epoch 40/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1639.6954 - g_loss: 7275.3693\n",
            "Epoch 41/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1580.6551 - g_loss: 6738.9120\n",
            "Epoch 42/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1629.7120 - g_loss: 6666.5087\n",
            "Epoch 43/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1697.3344 - g_loss: 6764.0468\n",
            "Epoch 44/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1626.4084 - g_loss: 6397.0029\n",
            "Epoch 45/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1540.8402 - g_loss: 7278.3814\n",
            "Epoch 46/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1547.9656 - g_loss: 6876.6078\n",
            "Epoch 47/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1619.4572 - g_loss: 7112.1569\n",
            "Epoch 48/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1594.8088 - g_loss: 7652.6974\n",
            "Epoch 49/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1711.1115 - g_loss: 7676.2741\n",
            "Epoch 50/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1590.8432 - g_loss: 7637.9881\n",
            "Epoch 51/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1469.9351 - g_loss: 7420.1239\n",
            "Epoch 52/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1558.4828 - g_loss: 7578.6101\n",
            "Epoch 53/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1425.4608 - g_loss: 7112.0885\n",
            "Epoch 54/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1578.4620 - g_loss: 6442.2535\n",
            "Epoch 55/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1644.7139 - g_loss: 6859.9700\n",
            "Epoch 56/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1544.7751 - g_loss: 6967.6731\n",
            "Epoch 57/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1451.4901 - g_loss: 6523.9954\n",
            "Epoch 58/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1504.6203 - g_loss: 6821.8537\n",
            "Epoch 59/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1629.0773 - g_loss: 7111.4303\n",
            "Epoch 60/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1378.6316 - g_loss: 6889.7279\n",
            "Epoch 61/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1408.5026 - g_loss: 7167.2593\n",
            "Epoch 62/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1491.0975 - g_loss: 8042.3169\n",
            "Epoch 63/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1490.5409 - g_loss: 8110.4217\n",
            "Epoch 64/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1445.5512 - g_loss: 9472.8196\n",
            "Epoch 65/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1520.6156 - g_loss: 10052.6010\n",
            "Epoch 66/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1382.3881 - g_loss: 10213.1986\n",
            "Epoch 67/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1544.4781 - g_loss: 10624.7608\n",
            "Epoch 68/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1440.4429 - g_loss: 10698.1538\n",
            "Epoch 69/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1569.0408 - g_loss: 10962.2973\n",
            "Epoch 70/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1516.5950 - g_loss: 11200.8449\n",
            "Epoch 71/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1555.9403 - g_loss: 12078.4215\n",
            "Epoch 72/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1406.9776 - g_loss: 10855.0572\n",
            "Epoch 73/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1427.6367 - g_loss: 11203.9062\n",
            "Epoch 74/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1430.4356 - g_loss: 11052.6268\n",
            "Epoch 75/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1368.9720 - g_loss: 9383.0497\n",
            "Epoch 76/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1378.4517 - g_loss: 9115.6733\n",
            "Epoch 77/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1618.9741 - g_loss: 9174.5296\n",
            "Epoch 78/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1460.0176 - g_loss: 9499.6870\n",
            "Epoch 79/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1324.4368 - g_loss: 9258.1021\n",
            "Epoch 80/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1406.2036 - g_loss: 8917.6221\n",
            "Epoch 81/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1444.8228 - g_loss: 8900.9597\n",
            "Epoch 82/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1428.6045 - g_loss: 8594.4908\n",
            "Epoch 83/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1312.4677 - g_loss: 8146.2003\n",
            "Epoch 84/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1337.6379 - g_loss: 7449.6312\n",
            "Epoch 85/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1534.5408 - g_loss: 7559.9750\n",
            "Epoch 86/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1332.3925 - g_loss: 7260.9306\n",
            "Epoch 87/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1418.1743 - g_loss: 6933.5255\n",
            "Epoch 88/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1282.6407 - g_loss: 7035.3051\n",
            "Epoch 89/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1344.3641 - g_loss: 6973.0515\n",
            "Epoch 90/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1380.8304 - g_loss: 6691.8823\n",
            "Epoch 91/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1330.7945 - g_loss: 6672.9393\n",
            "Epoch 92/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1344.3323 - g_loss: 7139.6110\n",
            "Epoch 93/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1334.2613 - g_loss: 6779.7026\n",
            "Epoch 94/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1289.8618 - g_loss: 6747.4745\n",
            "Epoch 95/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1325.9910 - g_loss: 5763.3292\n",
            "Epoch 96/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1301.8775 - g_loss: 5511.1492\n",
            "Epoch 97/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1345.2164 - g_loss: 5669.3681\n",
            "Epoch 98/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1333.0531 - g_loss: 5951.4515\n",
            "Epoch 99/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1355.2618 - g_loss: 6025.9763\n",
            "Epoch 100/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1289.7057 - g_loss: 6630.1213\n",
            "Epoch 101/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1242.5581 - g_loss: 6072.4942\n",
            "Epoch 102/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1150.8994 - g_loss: 5457.2677\n",
            "Epoch 103/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1186.9137 - g_loss: 5776.1339\n",
            "Epoch 104/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1149.4262 - g_loss: 5468.9017\n",
            "Epoch 105/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1216.6863 - g_loss: 5452.2308\n",
            "Epoch 106/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1289.8484 - g_loss: 5789.3314\n",
            "Epoch 107/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1340.4894 - g_loss: 5797.0549\n",
            "Epoch 108/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1151.8196 - g_loss: 4793.0638\n",
            "Epoch 109/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1212.0920 - g_loss: 5112.4206\n",
            "Epoch 110/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1092.7120 - g_loss: 4525.7643\n",
            "Epoch 111/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1176.2253 - g_loss: 4643.7795\n",
            "Epoch 112/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1043.3892 - g_loss: 4538.3989\n",
            "Epoch 113/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1195.9978 - g_loss: 4489.6914\n",
            "Epoch 114/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1161.3701 - g_loss: 4111.9335\n",
            "Epoch 115/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1190.9688 - g_loss: 4689.8799\n",
            "Epoch 116/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1166.9079 - g_loss: 4696.6493\n",
            "Epoch 117/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1089.1216 - g_loss: 4620.3707\n",
            "Epoch 118/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1117.3229 - g_loss: 4679.2358\n",
            "Epoch 119/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1265.5345 - g_loss: 4822.5825\n",
            "Epoch 120/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1108.0039 - g_loss: 4867.0172\n",
            "Epoch 121/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1209.0657 - g_loss: 5030.0453\n",
            "Epoch 122/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1056.5061 - g_loss: 5239.9555\n",
            "Epoch 123/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1217.7925 - g_loss: 5034.1194\n",
            "Epoch 124/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1118.0209 - g_loss: 4920.3342\n",
            "Epoch 125/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1087.8722 - g_loss: 4600.9842\n",
            "Epoch 126/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1133.1060 - g_loss: 4669.0639\n",
            "Epoch 127/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1054.5084 - g_loss: 4938.7542\n",
            "Epoch 128/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1032.0138 - g_loss: 4611.1188\n",
            "Epoch 129/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1256.4366 - g_loss: 5026.4274\n",
            "Epoch 130/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1118.4665 - g_loss: 4278.9329\n",
            "Epoch 131/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1058.1257 - g_loss: 4152.4943\n",
            "Epoch 132/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1202.8912 - g_loss: 4570.5919\n",
            "Epoch 133/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1124.0981 - g_loss: 3679.1111\n",
            "Epoch 134/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1118.6206 - g_loss: 4229.1574\n",
            "Epoch 135/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -993.5198 - g_loss: 4487.8215\n",
            "Epoch 136/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1043.2174 - g_loss: 4207.9729\n",
            "Epoch 137/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1108.1115 - g_loss: 4452.8558\n",
            "Epoch 138/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1064.1996 - g_loss: 5356.0669\n",
            "Epoch 139/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -997.2434 - g_loss: 4733.5547\n",
            "Epoch 140/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -942.7413 - g_loss: 4248.6731\n",
            "Epoch 141/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1037.9459 - g_loss: 3583.8809\n",
            "Epoch 142/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -960.8817 - g_loss: 4287.2990\n",
            "Epoch 143/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1069.6288 - g_loss: 4528.9991\n",
            "Epoch 144/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1063.6953 - g_loss: 4617.0106\n",
            "Epoch 145/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -998.9945 - g_loss: 4383.5946\n",
            "Epoch 146/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -948.8509 - g_loss: 5175.5633\n",
            "Epoch 147/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1012.6268 - g_loss: 4992.9141\n",
            "Epoch 148/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -939.4601 - g_loss: 5377.6663\n",
            "Epoch 149/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -917.2003 - g_loss: 4940.5001\n",
            "Epoch 150/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1039.3655 - g_loss: 4799.9018\n",
            "Epoch 151/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -946.2883 - g_loss: 4784.7780\n",
            "Epoch 152/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1025.7618 - g_loss: 3984.5203\n",
            "Epoch 153/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -942.8840 - g_loss: 4508.6533\n",
            "Epoch 154/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -904.7656 - g_loss: 3898.8798\n",
            "Epoch 155/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -966.5906 - g_loss: 3855.3760\n",
            "Epoch 156/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -941.9035 - g_loss: 3442.7484\n",
            "Epoch 157/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -907.2896 - g_loss: 3237.1256\n",
            "Epoch 158/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -871.9377 - g_loss: 3644.2952\n",
            "Epoch 159/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -968.9641 - g_loss: 3715.9413\n",
            "Epoch 160/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -903.8361 - g_loss: 3657.3676\n",
            "Epoch 161/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -981.8840 - g_loss: 2916.6592\n",
            "Epoch 162/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -928.6218 - g_loss: 3535.2652\n",
            "Epoch 163/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -890.8776 - g_loss: 3567.9595\n",
            "Epoch 164/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -870.3811 - g_loss: 3201.0425\n",
            "Epoch 165/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -959.0627 - g_loss: 3614.1967\n",
            "Epoch 166/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -904.8081 - g_loss: 3300.1560\n",
            "Epoch 167/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -860.9142 - g_loss: 3358.6400\n",
            "Epoch 168/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -888.0029 - g_loss: 3454.5588\n",
            "Epoch 169/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -895.4445 - g_loss: 3296.0389\n",
            "Epoch 170/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -826.0965 - g_loss: 3352.5719\n",
            "Epoch 171/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -840.6117 - g_loss: 3082.1693\n",
            "Epoch 172/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -927.9648 - g_loss: 2760.4192\n",
            "Epoch 173/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -935.7667 - g_loss: 2633.5510\n",
            "Epoch 174/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -889.4678 - g_loss: 3243.2927\n",
            "Epoch 175/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -816.9806 - g_loss: 3119.9936\n",
            "Epoch 176/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -790.0262 - g_loss: 3141.0291\n",
            "Epoch 177/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -825.3612 - g_loss: 2426.6976\n",
            "Epoch 178/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -916.9938 - g_loss: 2192.3771\n",
            "Epoch 179/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -868.6605 - g_loss: 2347.3814\n",
            "Epoch 180/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -854.9194 - g_loss: 2518.6650\n",
            "Epoch 181/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -888.8233 - g_loss: 3121.1207\n",
            "Epoch 182/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -869.3331 - g_loss: 2923.1058\n",
            "Epoch 183/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -805.7220 - g_loss: 2438.7701\n",
            "Epoch 184/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -781.1724 - g_loss: 1799.1982\n",
            "Epoch 185/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -930.7089 - g_loss: 2464.8127\n",
            "Epoch 186/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -864.7294 - g_loss: 3417.4318\n",
            "Epoch 187/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -807.6877 - g_loss: 3557.5345\n",
            "Epoch 188/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -755.5332 - g_loss: 2656.9606\n",
            "Epoch 189/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -741.5123 - g_loss: 2195.3079\n",
            "Epoch 190/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -788.3554 - g_loss: 2233.8249\n",
            "Epoch 191/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -770.0478 - g_loss: 1721.3465\n",
            "Epoch 192/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -809.6146 - g_loss: 1870.0811\n",
            "Epoch 193/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -730.1797 - g_loss: 1404.2443\n",
            "Epoch 194/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -759.2453 - g_loss: 1239.2102\n",
            "Epoch 195/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -718.8088 - g_loss: 1215.8028\n",
            "Epoch 196/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -707.5870 - g_loss: 1000.0529\n",
            "Epoch 197/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -731.3165 - g_loss: 1243.6273\n",
            "Epoch 198/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -731.0198 - g_loss: 188.2538\n",
            "Epoch 199/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -735.9367 - g_loss: -323.9047\n",
            "Epoch 200/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -723.4059 - g_loss: -87.6061\n",
            "Epoch 201/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -747.7792 - g_loss: -190.4410\n",
            "Epoch 202/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -756.0754 - g_loss: -419.9455\n",
            "Epoch 203/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -736.9194 - g_loss: -348.5668\n",
            "Epoch 204/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -727.1854 - g_loss: -1050.6247\n",
            "Epoch 205/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -698.9070 - g_loss: -580.0388\n",
            "Epoch 206/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -698.8064 - g_loss: -1683.8473\n",
            "Epoch 207/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -695.9781 - g_loss: -1568.4909\n",
            "Epoch 208/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -764.4199 - g_loss: -1318.4218\n",
            "Epoch 209/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -766.9558 - g_loss: -1662.3495\n",
            "Epoch 210/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -708.7865 - g_loss: -1323.5282\n",
            "Epoch 211/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -694.5065 - g_loss: -1005.4494\n",
            "Epoch 212/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -747.8523 - g_loss: -1050.3480\n",
            "Epoch 213/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -637.8658 - g_loss: -1761.2371\n",
            "Epoch 214/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -611.8482 - g_loss: -630.9222\n",
            "Epoch 215/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -754.4661 - g_loss: -1059.4098\n",
            "Epoch 216/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -635.7670 - g_loss: -1640.3062\n",
            "Epoch 217/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -707.5953 - g_loss: -2154.9254\n",
            "Epoch 218/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -639.5810 - g_loss: -2287.3949\n",
            "Epoch 219/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -697.4050 - g_loss: -2033.6059\n",
            "Epoch 220/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -660.5127 - g_loss: -2424.0944\n",
            "Epoch 221/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -612.9116 - g_loss: -3108.6411\n",
            "Epoch 222/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -648.0933 - g_loss: -2692.1752\n",
            "Epoch 223/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -694.7310 - g_loss: -2112.8190\n",
            "Epoch 224/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -633.7367 - g_loss: -2367.5136\n",
            "Epoch 225/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -593.9491 - g_loss: -3048.1635\n",
            "Epoch 226/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -688.1965 - g_loss: -3905.8782\n",
            "Epoch 227/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -713.7371 - g_loss: -3160.4327\n",
            "Epoch 228/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -699.2149 - g_loss: -1690.6729\n",
            "Epoch 229/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -576.1596 - g_loss: -2144.3112\n",
            "Epoch 230/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -514.1468 - g_loss: -2639.0150\n",
            "Epoch 231/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -616.6292 - g_loss: -3348.5419\n",
            "Epoch 232/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -545.8980 - g_loss: -3468.2833\n",
            "Epoch 233/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -612.2856 - g_loss: -2927.9661\n",
            "Epoch 234/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -669.3764 - g_loss: -3201.9837\n",
            "Epoch 235/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -664.7306 - g_loss: -2842.3892\n",
            "Epoch 236/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -521.2374 - g_loss: -2073.1641\n",
            "Epoch 237/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -599.9096 - g_loss: -1698.4459\n",
            "Epoch 238/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -556.7505 - g_loss: -1860.9913\n",
            "Epoch 239/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -632.0110 - g_loss: -1926.0655\n",
            "Epoch 240/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -597.3130 - g_loss: -1802.2941\n",
            "Epoch 241/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -524.9772 - g_loss: -2153.7132\n",
            "Epoch 242/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -586.5301 - g_loss: -2066.4773\n",
            "Epoch 243/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -600.8107 - g_loss: -2178.8787\n",
            "Epoch 244/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -622.5461 - g_loss: -2425.8746\n",
            "Epoch 245/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -526.6739 - g_loss: -2203.7201\n",
            "Epoch 246/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -532.2942 - g_loss: -2411.9025\n",
            "Epoch 247/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -561.2456 - g_loss: -2388.5444\n",
            "Epoch 248/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -551.3906 - g_loss: -2626.1768\n",
            "Epoch 249/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -632.6027 - g_loss: -2803.2198\n",
            "Epoch 250/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -544.0343 - g_loss: -2767.3468\n",
            "Epoch 251/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -604.3018 - g_loss: -2703.2658\n",
            "Epoch 252/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -636.7350 - g_loss: -2975.3416\n",
            "Epoch 253/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -511.4979 - g_loss: -2415.0902\n",
            "Epoch 254/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -548.5260 - g_loss: -2112.0543\n",
            "Epoch 255/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -534.8524 - g_loss: -2534.0604\n",
            "Epoch 256/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -542.2519 - g_loss: -2495.5123\n",
            "Epoch 257/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -525.1542 - g_loss: -1857.4598\n",
            "Epoch 258/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -531.3687 - g_loss: -2025.2245\n",
            "Epoch 259/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -553.5638 - g_loss: -2589.4202\n",
            "Epoch 260/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -544.1292 - g_loss: -3105.4184\n",
            "Epoch 261/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -555.8864 - g_loss: -2466.4019\n",
            "Epoch 262/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -598.9851 - g_loss: -2758.8736\n",
            "Epoch 263/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -519.6862 - g_loss: -1728.4810\n",
            "Epoch 264/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -545.1733 - g_loss: -2566.0113\n",
            "Epoch 265/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -482.6314 - g_loss: -3379.5217\n",
            "Epoch 266/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -508.1424 - g_loss: -2717.8199\n",
            "Epoch 267/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -477.7272 - g_loss: -2279.2930\n",
            "Epoch 268/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -491.5075 - g_loss: -2433.4751\n",
            "Epoch 269/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -465.5766 - g_loss: -2686.3781\n",
            "Epoch 270/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -570.7867 - g_loss: -2694.1032\n",
            "Epoch 271/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -494.9265 - g_loss: -2450.3922\n",
            "Epoch 272/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -428.5402 - g_loss: -2302.5462\n",
            "Epoch 273/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -462.4396 - g_loss: -2783.2521\n",
            "Epoch 274/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -540.4612 - g_loss: -2374.2487\n",
            "Epoch 275/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -452.3415 - g_loss: -2255.6199\n",
            "Epoch 276/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -548.9867 - g_loss: -2451.5039\n",
            "Epoch 277/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -531.6624 - g_loss: -2909.7416\n",
            "Epoch 278/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -488.1788 - g_loss: -2623.0504\n",
            "Epoch 279/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -665.5803 - g_loss: -2362.4577\n",
            "Epoch 280/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -367.0375 - g_loss: -1779.7503\n",
            "Epoch 281/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -612.2677 - g_loss: -2726.9136\n",
            "Epoch 282/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -467.6561 - g_loss: -1702.1095\n",
            "Epoch 283/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -487.0430 - g_loss: -1719.8687\n",
            "Epoch 284/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -552.7717 - g_loss: -1389.1802\n",
            "Epoch 285/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -336.9119 - g_loss: 218.0054\n",
            "Epoch 286/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -442.8229 - g_loss: 1355.9080\n",
            "Epoch 287/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -221.7016 - g_loss: 239.7610\n",
            "Epoch 288/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -550.2337 - g_loss: -481.6848\n",
            "Epoch 289/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -260.1254 - g_loss: -2880.0294\n",
            "Epoch 290/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -411.0557 - g_loss: -2864.9505\n",
            "Epoch 291/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -437.3309 - g_loss: -3509.5591\n",
            "Epoch 292/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -537.1905 - g_loss: -3443.2137\n",
            "Epoch 293/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -291.0310 - g_loss: -4059.7752\n",
            "Epoch 294/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -598.2145 - g_loss: -4055.2087\n",
            "Epoch 295/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -437.4192 - g_loss: -4243.2980\n",
            "Epoch 296/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -253.4576 - g_loss: -1901.8950\n",
            "Epoch 297/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -418.7850 - g_loss: -2390.3448\n",
            "Epoch 298/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -479.8964 - g_loss: -1426.5081\n",
            "Epoch 299/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -328.7637 - g_loss: -1122.1660\n",
            "Epoch 300/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -412.0851 - g_loss: -1439.6157\n",
            "Epoch 301/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -520.9368 - g_loss: -380.8537\n",
            "Epoch 302/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -420.7722 - g_loss: -1685.7102\n",
            "Epoch 303/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -204.2738 - g_loss: -3080.7247\n",
            "Epoch 304/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -505.6634 - g_loss: -1913.5007\n",
            "Epoch 305/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -538.6771 - g_loss: -2980.5352\n",
            "Epoch 306/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -530.3107 - g_loss: -4507.3189\n",
            "Epoch 307/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -209.8986 - g_loss: -2620.7413\n",
            "Epoch 308/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -538.5417 - g_loss: -3475.8391\n",
            "Epoch 309/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -285.1434 - g_loss: -2465.7641\n",
            "Epoch 310/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -460.2317 - g_loss: -2145.9074\n",
            "Epoch 311/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -481.5634 - g_loss: -4982.2293\n",
            "Epoch 312/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -423.8049 - g_loss: -3431.3792\n",
            "Epoch 313/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -376.9160 - g_loss: -3744.9968\n",
            "Epoch 314/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -252.2501 - g_loss: -2790.5323\n",
            "Epoch 315/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -495.7466 - g_loss: -1986.9488\n",
            "Epoch 316/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -434.6671 - g_loss: -1951.8378\n",
            "Epoch 317/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -281.3886 - g_loss: -3892.5297\n",
            "Epoch 318/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -352.6514 - g_loss: -967.5234\n",
            "Epoch 319/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -243.6192 - g_loss: -2114.6452\n",
            "Epoch 320/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -306.6687 - g_loss: -3246.7276\n",
            "Epoch 321/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -425.0685 - g_loss: -2477.8242\n",
            "Epoch 322/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -391.1735 - g_loss: -2228.7076\n",
            "Epoch 323/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -289.7592 - g_loss: -3425.3058\n",
            "Epoch 324/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -328.4615 - g_loss: -4770.8931\n",
            "Epoch 325/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -439.2435 - g_loss: -2530.1233\n",
            "Epoch 326/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -150.6225 - g_loss: -1806.5607\n",
            "Epoch 327/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -230.2030 - g_loss: 204.1216\n",
            "Epoch 328/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -322.8455 - g_loss: -62.0599\n",
            "Epoch 329/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -460.4302 - g_loss: -1719.9770\n",
            "Epoch 330/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -421.4986 - g_loss: -2043.6904\n",
            "Epoch 331/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -318.7415 - g_loss: -2927.4051\n",
            "Epoch 332/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -345.6807 - g_loss: -2435.8068\n",
            "Epoch 333/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -556.1708 - g_loss: -1859.6479\n",
            "Epoch 334/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -382.2392 - g_loss: -908.0863\n",
            "Epoch 335/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -313.1794 - g_loss: -532.3693\n",
            "Epoch 336/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -559.0471 - g_loss: -849.7271\n",
            "Epoch 337/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -583.9412 - g_loss: 818.4043\n",
            "Epoch 338/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -429.6146 - g_loss: -948.0294\n",
            "Epoch 339/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -386.0713 - g_loss: -614.9864\n",
            "Epoch 340/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -256.6656 - g_loss: -1677.2887\n",
            "Epoch 341/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -458.8338 - g_loss: -1661.7757\n",
            "Epoch 342/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -349.9748 - g_loss: -1276.5221\n",
            "Epoch 343/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -409.4497 - g_loss: -661.6824\n",
            "Epoch 344/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -275.5423 - g_loss: 537.9656\n",
            "Epoch 345/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -454.9172 - g_loss: 1265.7210\n",
            "Epoch 346/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -253.5051 - g_loss: 1111.0401\n",
            "Epoch 347/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -576.2539 - g_loss: -202.2615\n",
            "Epoch 348/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -281.0383 - g_loss: -811.6008\n",
            "Epoch 349/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -376.6297 - g_loss: -450.9494\n",
            "Epoch 350/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -205.3391 - g_loss: -760.4559\n",
            "Epoch 351/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -329.2750 - g_loss: -1724.9342\n",
            "Epoch 352/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -468.4561 - g_loss: -2597.2438\n",
            "Epoch 353/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -438.4187 - g_loss: -2938.0234\n",
            "Epoch 354/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -526.2382 - g_loss: -1651.3234\n",
            "Epoch 355/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -363.4172 - g_loss: -948.0658\n",
            "Epoch 356/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -539.7231 - g_loss: 117.1695\n",
            "Epoch 357/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -451.6200 - g_loss: 199.9463\n",
            "Epoch 358/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -297.0107 - g_loss: 1190.1698\n",
            "Epoch 359/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -338.3167 - g_loss: -746.0251\n",
            "Epoch 360/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -180.9344 - g_loss: 43.3469\n",
            "Epoch 361/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -494.3876 - g_loss: 1783.4260\n",
            "Epoch 362/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 118.1867 - g_loss: 2679.2210\n",
            "Epoch 363/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -444.6731 - g_loss: 4886.6538\n",
            "Epoch 364/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -435.7349 - g_loss: 3354.2673\n",
            "Epoch 365/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -172.1761 - g_loss: 758.5318\n",
            "Epoch 366/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -212.2856 - g_loss: 2210.9937\n",
            "Epoch 367/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -534.5208 - g_loss: 3188.8815\n",
            "Epoch 368/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -467.5887 - g_loss: 1762.5479\n",
            "Epoch 369/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -272.6935 - g_loss: 1441.7634\n",
            "Epoch 370/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -426.5488 - g_loss: 793.8406\n",
            "Epoch 371/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -200.9280 - g_loss: 1566.1160\n",
            "Epoch 372/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -229.5694 - g_loss: 1444.5560\n",
            "Epoch 373/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -12.5850 - g_loss: 810.4830\n",
            "Epoch 374/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -162.1167 - g_loss: 90.0618\n",
            "Epoch 375/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -66.7007 - g_loss: -2099.9999\n",
            "Epoch 376/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -218.7053 - g_loss: -3278.7850\n",
            "Epoch 377/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -139.5933 - g_loss: -4189.1223\n",
            "Epoch 378/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -370.4462 - g_loss: -4087.5855\n",
            "Epoch 379/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -168.0701 - g_loss: -2738.1022\n",
            "Epoch 380/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -524.5168 - g_loss: 1034.8911\n",
            "Epoch 381/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -325.2005 - g_loss: 1882.0915\n",
            "Epoch 382/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -309.5182 - g_loss: 837.8054\n",
            "Epoch 383/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -455.2713 - g_loss: -99.3842\n",
            "Epoch 384/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -50.2507 - g_loss: -1699.1441\n",
            "Epoch 385/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -200.0564 - g_loss: -364.4441\n",
            "Epoch 386/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -269.3495 - g_loss: -126.0637\n",
            "Epoch 387/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -301.2877 - g_loss: 1025.0588\n",
            "Epoch 388/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -349.2425 - g_loss: 937.5348\n",
            "Epoch 389/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -398.4004 - g_loss: 1445.4357\n",
            "Epoch 390/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -288.5946 - g_loss: 1809.9154\n",
            "Epoch 391/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -223.6066 - g_loss: 1646.0122\n",
            "Epoch 392/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -283.4432 - g_loss: 1586.0223\n",
            "Epoch 393/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -210.8660 - g_loss: 2283.4881\n",
            "Epoch 394/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -279.6958 - g_loss: 934.2961\n",
            "Epoch 395/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -358.1058 - g_loss: 1908.1242\n",
            "Epoch 396/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -340.7299 - g_loss: 1011.6430\n",
            "Epoch 397/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -394.3017 - g_loss: 2004.9660\n",
            "Epoch 398/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -325.3972 - g_loss: 2027.4363\n",
            "Epoch 399/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -287.4911 - g_loss: 777.1399\n",
            "Epoch 400/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -200.8714 - g_loss: 3019.9262\n",
            "Epoch 401/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -195.8446 - g_loss: 3168.0839\n",
            "Epoch 402/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -188.3271 - g_loss: 3229.2162\n",
            "Epoch 403/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -393.9586 - g_loss: 1705.2189\n",
            "Epoch 404/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -126.5075 - g_loss: 842.3277\n",
            "Epoch 405/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -215.5831 - g_loss: 127.0066\n",
            "Epoch 406/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -386.8371 - g_loss: 527.9220\n",
            "Epoch 407/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -122.0944 - g_loss: -24.7877\n",
            "Epoch 408/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -98.1967 - g_loss: 839.2595\n",
            "Epoch 409/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -263.7928 - g_loss: -1166.6326\n",
            "Epoch 410/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -130.3117 - g_loss: -2316.7955\n",
            "Epoch 411/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -374.6010 - g_loss: -820.5295\n",
            "Epoch 412/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -413.3469 - g_loss: -1018.5904\n",
            "Epoch 413/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -198.2711 - g_loss: -2775.6676\n",
            "Epoch 414/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -240.4823 - g_loss: 492.4737\n",
            "Epoch 415/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -597.9526 - g_loss: 1374.0822\n",
            "Epoch 416/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -248.3036 - g_loss: 172.6995\n",
            "Epoch 417/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -588.7493 - g_loss: 221.9792\n",
            "Epoch 418/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -232.7827 - g_loss: 2172.4490\n",
            "Epoch 419/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -552.4665 - g_loss: 1434.0177\n",
            "Epoch 420/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -416.6826 - g_loss: 3037.2289\n",
            "Epoch 421/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 362.4238 - g_loss: 1160.2287\n",
            "Epoch 422/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -151.6472 - g_loss: 294.4204\n",
            "Epoch 423/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 55.9872 - g_loss: 1693.0628\n",
            "Epoch 424/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 176.1990 - g_loss: 1229.6183\n",
            "Epoch 425/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -163.6917 - g_loss: 1484.7477\n",
            "Epoch 426/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -175.8802 - g_loss: 1463.7244\n",
            "Epoch 427/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -143.9758 - g_loss: 578.4310\n",
            "Epoch 428/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -363.4542 - g_loss: 2108.0678\n",
            "Epoch 429/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -127.0800 - g_loss: 2767.5394\n",
            "Epoch 430/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -136.5110 - g_loss: 3320.3921\n",
            "Epoch 431/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -316.6660 - g_loss: 2088.4305\n",
            "Epoch 432/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -215.7154 - g_loss: 2316.9964\n",
            "Epoch 433/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -282.6154 - g_loss: 1372.8273\n",
            "Epoch 434/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -243.2634 - g_loss: 2629.4437\n",
            "Epoch 435/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -232.8133 - g_loss: 2249.1255\n",
            "Epoch 436/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -306.0940 - g_loss: 2471.8101\n",
            "Epoch 437/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -293.8200 - g_loss: 2847.1078\n",
            "Epoch 438/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -119.6990 - g_loss: 1649.4592\n",
            "Epoch 439/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -182.1691 - g_loss: 1325.1172\n",
            "Epoch 440/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -294.9755 - g_loss: 870.5925\n",
            "Epoch 441/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -143.2884 - g_loss: 1304.6731\n",
            "Epoch 442/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -192.0013 - g_loss: 665.5809\n",
            "Epoch 443/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -214.2095 - g_loss: 733.4829\n",
            "Epoch 444/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -212.8007 - g_loss: 276.9887\n",
            "Epoch 445/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -256.8711 - g_loss: 1459.6173\n",
            "Epoch 446/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -142.5964 - g_loss: 1607.7272\n",
            "Epoch 447/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -243.2636 - g_loss: 1878.7698\n",
            "Epoch 448/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -193.5461 - g_loss: 2898.5718\n",
            "Epoch 449/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -172.5162 - g_loss: 2073.0626\n",
            "Epoch 450/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -268.1425 - g_loss: 1692.5008\n",
            "Epoch 451/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -174.1468 - g_loss: 1414.7289\n",
            "Epoch 452/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -230.7575 - g_loss: 1104.1374\n",
            "Epoch 453/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -238.5030 - g_loss: 1363.8666\n",
            "Epoch 454/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -168.2863 - g_loss: 1675.7056\n",
            "Epoch 455/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -269.3755 - g_loss: 2091.5209\n",
            "Epoch 456/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -240.5162 - g_loss: 2418.1322\n",
            "Epoch 457/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -212.3194 - g_loss: 3175.8099\n",
            "Epoch 458/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -229.5589 - g_loss: 3650.3693\n",
            "Epoch 459/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -195.4908 - g_loss: 3122.2470\n",
            "Epoch 460/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -268.6983 - g_loss: 3677.3385\n",
            "Epoch 461/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -197.0529 - g_loss: 2968.9454\n",
            "Epoch 462/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -234.9101 - g_loss: 4169.2573\n",
            "Epoch 463/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -149.4890 - g_loss: 4508.7438\n",
            "Epoch 464/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -150.4694 - g_loss: 4400.7703\n",
            "Epoch 465/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -217.6083 - g_loss: 4348.4636\n",
            "Epoch 466/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -270.4938 - g_loss: 5085.0850\n",
            "Epoch 467/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -245.4379 - g_loss: 5487.8023\n",
            "Epoch 468/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -214.8061 - g_loss: 4461.6581\n",
            "Epoch 469/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -230.6711 - g_loss: 4237.2493\n",
            "Epoch 470/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -261.0149 - g_loss: 3486.0326\n",
            "Epoch 471/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -153.4536 - g_loss: 3844.9572\n",
            "Epoch 472/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -159.8138 - g_loss: 3568.0193\n",
            "Epoch 473/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -232.7691 - g_loss: 3201.0020\n",
            "Epoch 474/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -172.5733 - g_loss: 3176.8935\n",
            "Epoch 475/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -192.0479 - g_loss: 3089.1948\n",
            "Epoch 476/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -230.9861 - g_loss: 3491.6348\n",
            "Epoch 477/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -237.4251 - g_loss: 3784.1424\n",
            "Epoch 478/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -150.6235 - g_loss: 2541.4143\n",
            "Epoch 479/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -185.0314 - g_loss: 2171.5083\n",
            "Epoch 480/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -166.8036 - g_loss: 1966.6332\n",
            "Epoch 481/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -171.9153 - g_loss: 1778.7002\n",
            "Epoch 482/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -193.9294 - g_loss: 2327.2028\n",
            "Epoch 483/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -249.3318 - g_loss: 2410.5875\n",
            "Epoch 484/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -193.2491 - g_loss: 2257.7113\n",
            "Epoch 485/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -185.8165 - g_loss: 2400.4628\n",
            "Epoch 486/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -243.5924 - g_loss: 2455.9568\n",
            "Epoch 487/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -107.5210 - g_loss: 3075.5105\n",
            "Epoch 488/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -166.7221 - g_loss: 2591.7772\n",
            "Epoch 489/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -197.5179 - g_loss: 3009.3835\n",
            "Epoch 490/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -200.2198 - g_loss: 3523.6468\n",
            "Epoch 491/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -141.8649 - g_loss: 2802.5815\n",
            "Epoch 492/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -136.0514 - g_loss: 2187.9066\n",
            "Epoch 493/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -215.8948 - g_loss: 2658.8388\n",
            "Epoch 494/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -171.1456 - g_loss: 2321.4571\n",
            "Epoch 495/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -199.7877 - g_loss: 2696.8832\n",
            "Epoch 496/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -196.2202 - g_loss: 3123.4512\n",
            "Epoch 497/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -197.4991 - g_loss: 2792.6761\n",
            "Epoch 498/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -136.7621 - g_loss: 2514.9202\n",
            "Epoch 499/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -201.2224 - g_loss: 2684.9594\n",
            "Epoch 500/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -150.6416 - g_loss: 2470.9114\n",
            "Epoch 501/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -167.8259 - g_loss: 2406.4617\n",
            "Epoch 502/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -168.1145 - g_loss: 2215.8639\n",
            "Epoch 503/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -153.4118 - g_loss: 2005.5335\n",
            "Epoch 504/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -162.5827 - g_loss: 1942.4388\n",
            "Epoch 505/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -176.2262 - g_loss: 2113.8978\n",
            "Epoch 506/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -170.2023 - g_loss: 2182.9639\n",
            "Epoch 507/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -184.1565 - g_loss: 2735.6001\n",
            "Epoch 508/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -164.1798 - g_loss: 2260.7596\n",
            "Epoch 509/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -182.1916 - g_loss: 2447.0448\n",
            "Epoch 510/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -190.4342 - g_loss: 3362.5775\n",
            "Epoch 511/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -175.8630 - g_loss: 3640.9115\n",
            "Epoch 512/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -166.0094 - g_loss: 3376.7134\n",
            "Epoch 513/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -169.5736 - g_loss: 3259.1921\n",
            "Epoch 514/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -190.2154 - g_loss: 3215.8649\n",
            "Epoch 515/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -169.4419 - g_loss: 2954.2053\n",
            "Epoch 516/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -127.4749 - g_loss: 3326.3559\n",
            "Epoch 517/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -157.3757 - g_loss: 3178.9256\n",
            "Epoch 518/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -173.5008 - g_loss: 3292.1167\n",
            "Epoch 519/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -157.6373 - g_loss: 3085.5399\n",
            "Epoch 520/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -162.4320 - g_loss: 3088.0941\n",
            "Epoch 521/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -160.7642 - g_loss: 3162.6945\n",
            "Epoch 522/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -174.8993 - g_loss: 3198.7457\n",
            "Epoch 523/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -152.8872 - g_loss: 2777.9978\n",
            "Epoch 524/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -153.5825 - g_loss: 3149.7816\n",
            "Epoch 525/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -170.6845 - g_loss: 3425.7769\n",
            "Epoch 526/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -166.8079 - g_loss: 3799.0999\n",
            "Epoch 527/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -139.9572 - g_loss: 4061.6125\n",
            "Epoch 528/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -171.7174 - g_loss: 3908.0079\n",
            "Epoch 529/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -144.6313 - g_loss: 4572.0439\n",
            "Epoch 530/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -143.5009 - g_loss: 4960.7180\n",
            "Epoch 531/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -120.2535 - g_loss: 4144.5217\n",
            "Epoch 532/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -133.0917 - g_loss: 3607.0564\n",
            "Epoch 533/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -144.8065 - g_loss: 3801.7979\n",
            "Epoch 534/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -151.7856 - g_loss: 3683.0607\n",
            "Epoch 535/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -153.1513 - g_loss: 2718.9904\n",
            "Epoch 536/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -172.7168 - g_loss: 3024.4274\n",
            "Epoch 537/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -145.7258 - g_loss: 2702.5772\n",
            "Epoch 538/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -144.9030 - g_loss: 2602.9408\n",
            "Epoch 539/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -141.1442 - g_loss: 2795.5744\n",
            "Epoch 540/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -137.2644 - g_loss: 2698.7978\n",
            "Epoch 541/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -137.4786 - g_loss: 2247.2148\n",
            "Epoch 542/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -135.4367 - g_loss: 2221.3835\n",
            "Epoch 543/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -151.0355 - g_loss: 1990.7575\n",
            "Epoch 544/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -127.1024 - g_loss: 2192.5095\n",
            "Epoch 545/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -132.4600 - g_loss: 2088.5223\n",
            "Epoch 546/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -141.7702 - g_loss: 1985.8719\n",
            "Epoch 547/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -141.6763 - g_loss: 1712.9271\n",
            "Epoch 548/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -132.9614 - g_loss: 2019.7696\n",
            "Epoch 549/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -138.7879 - g_loss: 2184.2836\n",
            "Epoch 550/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -143.9806 - g_loss: 2327.5486\n",
            "Epoch 551/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -120.9991 - g_loss: 1705.6342\n",
            "Epoch 552/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -140.3409 - g_loss: 1519.1392\n",
            "Epoch 553/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -122.6835 - g_loss: 1999.3158\n",
            "Epoch 554/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -123.4007 - g_loss: 1844.3439\n",
            "Epoch 555/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -139.1677 - g_loss: 1865.7787\n",
            "Epoch 556/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -151.0284 - g_loss: 2150.4021\n",
            "Epoch 557/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -140.1344 - g_loss: 2237.2603\n",
            "Epoch 558/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -131.6382 - g_loss: 1831.4652\n",
            "Epoch 559/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -132.1292 - g_loss: 1860.3220\n",
            "Epoch 560/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -135.3690 - g_loss: 1607.5623\n",
            "Epoch 561/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -80.8607 - g_loss: 1479.2141\n",
            "Epoch 562/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -115.9179 - g_loss: 1678.4972\n",
            "Epoch 563/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -106.5869 - g_loss: 2177.9720\n",
            "Epoch 564/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -120.1560 - g_loss: 2322.5686\n",
            "Epoch 565/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -108.6445 - g_loss: 2290.4450\n",
            "Epoch 566/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -113.3285 - g_loss: 2693.3623\n",
            "Epoch 567/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -131.4854 - g_loss: 3582.6367\n",
            "Epoch 568/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -101.2136 - g_loss: 4689.6243\n",
            "Epoch 569/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -113.3185 - g_loss: 5241.4540\n",
            "Epoch 570/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -134.2511 - g_loss: 5580.2032\n",
            "Epoch 571/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -132.0304 - g_loss: 4820.5230\n",
            "Epoch 572/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -96.6113 - g_loss: 3870.1078\n",
            "Epoch 573/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -142.1389 - g_loss: 4412.8476\n",
            "Epoch 574/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -106.5589 - g_loss: 6110.4654\n",
            "Epoch 575/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -93.2798 - g_loss: 4932.3381\n",
            "Epoch 576/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -101.1393 - g_loss: 2914.7604\n",
            "Epoch 577/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -132.8169 - g_loss: 4761.8075\n",
            "Epoch 578/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -137.3488 - g_loss: 5008.9362\n",
            "Epoch 579/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -93.7028 - g_loss: 4848.8027\n",
            "Epoch 580/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -127.4962 - g_loss: 4595.6229\n",
            "Epoch 581/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -119.3941 - g_loss: 4478.8200\n",
            "Epoch 582/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -117.4234 - g_loss: 4831.4478\n",
            "Epoch 583/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -90.4965 - g_loss: 4385.8906\n",
            "Epoch 584/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -107.5426 - g_loss: 2939.9192\n",
            "Epoch 585/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -68.3983 - g_loss: 2591.7358\n",
            "Epoch 586/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -88.4112 - g_loss: 2024.2924\n",
            "Epoch 587/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -111.4174 - g_loss: 2370.0172\n",
            "Epoch 588/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -122.7586 - g_loss: 3506.9363\n",
            "Epoch 589/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -111.4156 - g_loss: 3826.8767\n",
            "Epoch 590/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -107.0431 - g_loss: 4199.4031\n",
            "Epoch 591/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -107.1149 - g_loss: 2892.0286\n",
            "Epoch 592/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -110.4590 - g_loss: 3320.2509\n",
            "Epoch 593/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -94.2219 - g_loss: 3853.7737\n",
            "Epoch 594/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -84.7421 - g_loss: 3066.3147\n",
            "Epoch 595/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -82.8723 - g_loss: 2227.2640\n",
            "Epoch 596/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -95.3284 - g_loss: 1388.3857\n",
            "Epoch 597/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -101.1280 - g_loss: 1739.3838\n",
            "Epoch 598/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -102.3928 - g_loss: 1811.1006\n",
            "Epoch 599/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -98.7262 - g_loss: 1645.7831\n",
            "Epoch 600/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -107.2526 - g_loss: 1694.4359\n",
            "Epoch 601/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -91.0330 - g_loss: 2035.8451\n",
            "Epoch 602/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -105.5136 - g_loss: 2497.7111\n",
            "Epoch 603/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -108.5441 - g_loss: 2565.5509\n",
            "Epoch 604/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -101.0586 - g_loss: 1984.8347\n",
            "Epoch 605/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -82.7497 - g_loss: 1257.6209\n",
            "Epoch 606/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -103.9485 - g_loss: 1431.6337\n",
            "Epoch 607/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -86.6416 - g_loss: 1329.4179\n",
            "Epoch 608/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -102.6875 - g_loss: 1449.4833\n",
            "Epoch 609/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -97.9539 - g_loss: 1526.2499\n",
            "Epoch 610/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -97.5141 - g_loss: 1623.8380\n",
            "Epoch 611/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -106.1484 - g_loss: 1677.0218\n",
            "Epoch 612/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -100.0016 - g_loss: 1873.3197\n",
            "Epoch 613/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -79.6503 - g_loss: 1815.2352\n",
            "Epoch 614/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -89.5701 - g_loss: 1821.0719\n",
            "Epoch 615/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -95.8414 - g_loss: 1880.4527\n",
            "Epoch 616/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -108.4964 - g_loss: 2326.6529\n",
            "Epoch 617/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -119.8147 - g_loss: 3451.8236\n",
            "Epoch 618/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -98.4580 - g_loss: 3560.2308\n",
            "Epoch 619/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -105.1638 - g_loss: 3029.5874\n",
            "Epoch 620/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -102.0779 - g_loss: 3035.9580\n",
            "Epoch 621/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -114.1525 - g_loss: 3066.6829\n",
            "Epoch 622/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -89.4136 - g_loss: 2695.1067\n",
            "Epoch 623/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -96.6129 - g_loss: 3015.1708\n",
            "Epoch 624/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -101.6966 - g_loss: 3366.0152\n",
            "Epoch 625/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -72.2823 - g_loss: 2987.1496\n",
            "Epoch 626/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -95.6974 - g_loss: 2331.8558\n",
            "Epoch 627/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -93.9897 - g_loss: 2099.8991\n",
            "Epoch 628/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -82.5185 - g_loss: 1968.4091\n",
            "Epoch 629/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -89.5938 - g_loss: 2478.9293\n",
            "Epoch 630/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -108.0425 - g_loss: 2829.3899\n",
            "Epoch 631/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -98.3494 - g_loss: 2866.1415\n",
            "Epoch 632/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -97.8211 - g_loss: 2718.3470\n",
            "Epoch 633/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -84.4555 - g_loss: 3168.2060\n",
            "Epoch 634/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -97.5846 - g_loss: 3211.6356\n",
            "Epoch 635/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -92.9683 - g_loss: 3111.0719\n",
            "Epoch 636/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -76.7661 - g_loss: 3347.6186\n",
            "Epoch 637/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -77.0107 - g_loss: 4140.0736\n",
            "Epoch 638/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -97.2621 - g_loss: 4303.3557\n",
            "Epoch 639/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -88.4428 - g_loss: 3119.5113\n",
            "Epoch 640/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -77.1086 - g_loss: 2688.8173\n",
            "Epoch 641/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -91.2390 - g_loss: 2069.9010\n",
            "Epoch 642/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -80.5481 - g_loss: 1889.0439\n",
            "Epoch 643/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -85.9361 - g_loss: 1528.1731\n",
            "Epoch 644/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -88.8493 - g_loss: 1518.9064\n",
            "Epoch 645/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -93.4312 - g_loss: 1673.3562\n",
            "Epoch 646/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -80.6281 - g_loss: 1691.8874\n",
            "Epoch 647/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -83.1453 - g_loss: 1743.2997\n",
            "Epoch 648/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -91.1598 - g_loss: 1572.2939\n",
            "Epoch 649/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -72.4976 - g_loss: 1503.3911\n",
            "Epoch 650/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -91.7533 - g_loss: 1578.1725\n",
            "Epoch 651/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -79.5586 - g_loss: 1702.7074\n",
            "Epoch 652/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -78.6866 - g_loss: 1305.9986\n",
            "Epoch 653/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -78.3169 - g_loss: 1323.7659\n",
            "Epoch 654/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -93.4174 - g_loss: 1523.0315\n",
            "Epoch 655/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -90.4418 - g_loss: 1663.6267\n",
            "Epoch 656/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -77.0242 - g_loss: 1548.9203\n",
            "Epoch 657/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -83.8963 - g_loss: 1559.6312\n",
            "Epoch 658/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -77.6401 - g_loss: 1475.4530\n",
            "Epoch 659/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -77.6693 - g_loss: 1113.9661\n",
            "Epoch 660/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -64.0376 - g_loss: 747.9725\n",
            "Epoch 661/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -71.4325 - g_loss: 991.3525\n",
            "Epoch 662/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -57.4264 - g_loss: 1012.9604\n",
            "Epoch 663/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -70.3347 - g_loss: 1839.4822\n",
            "Epoch 664/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -51.1142 - g_loss: 1908.2217\n",
            "Epoch 665/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -63.4432 - g_loss: 1446.6384\n",
            "Epoch 666/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -71.8440 - g_loss: 1520.6117\n",
            "Epoch 667/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -62.4268 - g_loss: 1928.2508\n",
            "Epoch 668/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -83.2103 - g_loss: 2529.9708\n",
            "Epoch 669/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -68.4827 - g_loss: 2563.4708\n",
            "Epoch 670/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -70.1075 - g_loss: 2461.0943\n",
            "Epoch 671/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -74.5039 - g_loss: 2464.3940\n",
            "Epoch 672/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -65.7883 - g_loss: 2577.6069\n",
            "Epoch 673/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -49.2782 - g_loss: 2196.6562\n",
            "Epoch 674/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -61.4219 - g_loss: 1853.3161\n",
            "Epoch 675/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -67.5347 - g_loss: 2461.0335\n",
            "Epoch 676/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -70.8272 - g_loss: 2534.2709\n",
            "Epoch 677/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -57.9058 - g_loss: 3308.6948\n",
            "Epoch 678/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -87.4420 - g_loss: 3357.3182\n",
            "Epoch 679/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -36.1567 - g_loss: 2895.2323\n",
            "Epoch 680/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -54.0609 - g_loss: 2170.8253\n",
            "Epoch 681/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -86.3585 - g_loss: 2568.7581\n",
            "Epoch 682/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -60.9132 - g_loss: 2206.4410\n",
            "Epoch 683/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -77.7088 - g_loss: 2049.7788\n",
            "Epoch 684/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -56.1773 - g_loss: 2007.0548\n",
            "Epoch 685/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -70.9194 - g_loss: 1737.6241\n",
            "Epoch 686/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -69.8277 - g_loss: 1539.6993\n",
            "Epoch 687/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -46.3923 - g_loss: 2185.5915\n",
            "Epoch 688/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -47.0367 - g_loss: 2360.7640\n",
            "Epoch 689/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -58.8109 - g_loss: 2183.5974\n",
            "Epoch 690/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -50.2914 - g_loss: 2438.2731\n",
            "Epoch 691/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -70.7806 - g_loss: 2062.5201\n",
            "Epoch 692/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -93.2114 - g_loss: 2194.6733\n",
            "Epoch 693/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -74.7136 - g_loss: 2091.9213\n",
            "Epoch 694/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -58.7472 - g_loss: 2171.4646\n",
            "Epoch 695/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -65.4728 - g_loss: 2063.5882\n",
            "Epoch 696/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -70.8429 - g_loss: 2358.7671\n",
            "Epoch 697/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -73.5300 - g_loss: 2004.3142\n",
            "Epoch 698/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -59.4143 - g_loss: 1832.3041\n",
            "Epoch 699/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -61.0517 - g_loss: 1740.3278\n",
            "Epoch 700/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -59.5543 - g_loss: 1831.1920\n",
            "Epoch 701/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -70.6937 - g_loss: 1999.0558\n",
            "Epoch 702/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -54.9957 - g_loss: 1622.8770\n",
            "Epoch 703/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -60.3474 - g_loss: 1260.3159\n",
            "Epoch 704/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -63.3740 - g_loss: 1193.4389\n",
            "Epoch 705/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -60.0821 - g_loss: 1690.1884\n",
            "Epoch 706/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -75.4810 - g_loss: 1700.8265\n",
            "Epoch 707/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -49.3379 - g_loss: 1157.6469\n",
            "Epoch 708/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -65.3125 - g_loss: 1050.7300\n",
            "Epoch 709/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -58.5822 - g_loss: 1069.1553\n",
            "Epoch 710/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -70.7757 - g_loss: 1401.6528\n",
            "Epoch 711/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -60.5679 - g_loss: 1411.1854\n",
            "Epoch 712/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -62.9418 - g_loss: 1349.9490\n",
            "Epoch 713/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -60.1795 - g_loss: 1164.2245\n",
            "Epoch 714/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -70.0215 - g_loss: 1469.0611\n",
            "Epoch 715/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -65.2820 - g_loss: 1578.6305\n",
            "Epoch 716/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -59.7732 - g_loss: 1376.9665\n",
            "Epoch 717/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -53.6842 - g_loss: 1514.0012\n",
            "Epoch 718/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -76.1791 - g_loss: 1651.8116\n",
            "Epoch 719/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -65.7179 - g_loss: 1794.2486\n",
            "Epoch 720/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -60.8925 - g_loss: 1462.1686\n",
            "Epoch 721/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -56.6004 - g_loss: 1504.2182\n",
            "Epoch 722/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -65.4559 - g_loss: 1379.3047\n",
            "Epoch 723/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -55.8470 - g_loss: 1083.4241\n",
            "Epoch 724/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -52.1665 - g_loss: 1102.9462\n",
            "Epoch 725/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -64.6400 - g_loss: 1018.5881\n",
            "Epoch 726/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -56.8846 - g_loss: 1070.8500\n",
            "Epoch 727/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -53.4667 - g_loss: 836.7933\n",
            "Epoch 728/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -65.2508 - g_loss: 1141.0779\n",
            "Epoch 729/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -59.6592 - g_loss: 1234.7892\n",
            "Epoch 730/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -69.4919 - g_loss: 1214.3822\n",
            "Epoch 731/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -66.2850 - g_loss: 1147.0465\n",
            "Epoch 732/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -57.1189 - g_loss: 1238.8160\n",
            "Epoch 733/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -43.6937 - g_loss: 1058.1160\n",
            "Epoch 734/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -52.0337 - g_loss: 849.1972\n",
            "Epoch 735/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -56.6588 - g_loss: 844.4932\n",
            "Epoch 736/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -63.0292 - g_loss: 904.5571\n",
            "Epoch 737/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -56.2153 - g_loss: 1087.3031\n",
            "Epoch 738/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -41.9058 - g_loss: 940.5068\n",
            "Epoch 739/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -59.3061 - g_loss: 718.8606\n",
            "Epoch 740/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -54.0737 - g_loss: 633.1459\n",
            "Epoch 741/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -54.5915 - g_loss: 869.8632\n",
            "Epoch 742/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -47.4934 - g_loss: 874.0838\n",
            "Epoch 743/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -65.6616 - g_loss: 906.6270\n",
            "Epoch 744/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -69.2437 - g_loss: 1330.2887\n",
            "Epoch 745/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -51.8052 - g_loss: 1278.8639\n",
            "Epoch 746/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -62.6959 - g_loss: 1142.9671\n",
            "Epoch 747/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -53.1401 - g_loss: 1225.0211\n",
            "Epoch 748/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -57.0783 - g_loss: 1371.2341\n",
            "Epoch 749/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -56.7961 - g_loss: 1611.7393\n",
            "Epoch 750/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -48.4741 - g_loss: 1520.0457\n",
            "Epoch 751/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -40.6810 - g_loss: 1518.0272\n",
            "Epoch 752/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -58.1192 - g_loss: 1784.6749\n",
            "Epoch 753/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -56.3274 - g_loss: 1663.8403\n",
            "Epoch 754/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -33.6942 - g_loss: 1343.0602\n",
            "Epoch 755/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -49.1783 - g_loss: 1245.8924\n",
            "Epoch 756/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -48.6716 - g_loss: 1172.5576\n",
            "Epoch 757/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -45.6927 - g_loss: 870.4525\n",
            "Epoch 758/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -52.3977 - g_loss: 881.3187\n",
            "Epoch 759/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -45.2827 - g_loss: 1184.6593\n",
            "Epoch 760/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -43.2363 - g_loss: 1278.6424\n",
            "Epoch 761/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -51.2930 - g_loss: 1262.8754\n",
            "Epoch 762/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -38.4561 - g_loss: 964.3440\n",
            "Epoch 763/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -37.5805 - g_loss: 783.5752\n",
            "Epoch 764/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -38.7296 - g_loss: 1117.0191\n",
            "Epoch 765/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -36.2092 - g_loss: 834.4624\n",
            "Epoch 766/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -46.4432 - g_loss: 978.0698\n",
            "Epoch 767/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -41.8944 - g_loss: 902.8641\n",
            "Epoch 768/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -38.4123 - g_loss: 1109.1257\n",
            "Epoch 769/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -35.4820 - g_loss: 1270.7961\n",
            "Epoch 770/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -42.0068 - g_loss: 1110.8453\n",
            "Epoch 771/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -36.6189 - g_loss: 1596.3749\n",
            "Epoch 772/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -24.5374 - g_loss: 1838.6692\n",
            "Epoch 773/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -55.3328 - g_loss: 2076.2550\n",
            "Epoch 774/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -60.9604 - g_loss: 1570.8244\n",
            "Epoch 775/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -40.9377 - g_loss: 1457.7959\n",
            "Epoch 776/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -40.5806 - g_loss: 1635.6410\n",
            "Epoch 777/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -40.4058 - g_loss: 1683.1922\n",
            "Epoch 778/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -54.1662 - g_loss: 1310.6994\n",
            "Epoch 779/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -40.7842 - g_loss: 1664.2138\n",
            "Epoch 780/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -40.2420 - g_loss: 1101.2107\n",
            "Epoch 781/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -36.1099 - g_loss: 1876.1307\n",
            "Epoch 782/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -38.1997 - g_loss: 1880.1440\n",
            "Epoch 783/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -32.0464 - g_loss: 1749.1065\n",
            "Epoch 784/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -34.9648 - g_loss: 1852.5542\n",
            "Epoch 785/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -43.1868 - g_loss: 2095.0146\n",
            "Epoch 786/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -54.4240 - g_loss: 2047.5423\n",
            "Epoch 787/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -46.3963 - g_loss: 1701.1972\n",
            "Epoch 788/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -19.2182 - g_loss: 2447.7732\n",
            "Epoch 789/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -32.1101 - g_loss: 1761.0705\n",
            "Epoch 790/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -44.6294 - g_loss: 2343.4575\n",
            "Epoch 791/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -92.6382 - g_loss: 2893.4167\n",
            "Epoch 792/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -51.4551 - g_loss: 3906.0575\n",
            "Epoch 793/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 5.1171 - g_loss: 2143.8714\n",
            "Epoch 794/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -38.2373 - g_loss: 1970.0195\n",
            "Epoch 795/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -35.2126 - g_loss: 1484.5479\n",
            "Epoch 796/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -35.5858 - g_loss: 1430.3642\n",
            "Epoch 797/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -44.4246 - g_loss: 1775.5367\n",
            "Epoch 798/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -47.9025 - g_loss: 2245.9634\n",
            "Epoch 799/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -33.6833 - g_loss: 2309.8767\n",
            "Epoch 800/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -28.4595 - g_loss: 2153.9045\n",
            "Epoch 801/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -36.8408 - g_loss: 1666.9740\n",
            "Epoch 802/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -35.3668 - g_loss: 1455.5290\n",
            "Epoch 803/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -36.3793 - g_loss: 1749.5393\n",
            "Epoch 804/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -30.9934 - g_loss: 1605.6962\n",
            "Epoch 805/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -49.7186 - g_loss: 1943.5313\n",
            "Epoch 806/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -26.4358 - g_loss: 2330.8012\n",
            "Epoch 807/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -53.4347 - g_loss: 2863.5171\n",
            "Epoch 808/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -37.0451 - g_loss: 2980.7188\n",
            "Epoch 809/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1.3235 - g_loss: 2829.5575\n",
            "Epoch 810/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -26.9717 - g_loss: 1620.3123\n",
            "Epoch 811/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -27.6861 - g_loss: 1813.0607\n",
            "Epoch 812/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -49.6363 - g_loss: 3169.9118\n",
            "Epoch 813/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -40.9492 - g_loss: 3639.2811\n",
            "Epoch 814/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -60.2106 - g_loss: 4361.3061\n",
            "Epoch 815/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -87.3404 - g_loss: 5817.3634\n",
            "Epoch 816/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -17.1633 - g_loss: 6293.9845\n",
            "Epoch 817/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -87.2928 - g_loss: 6120.3703\n",
            "Epoch 818/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -104.4487 - g_loss: 6475.7507\n",
            "Epoch 819/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -84.9901 - g_loss: 7955.5346\n",
            "Epoch 820/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -18.8779 - g_loss: 7747.2731\n",
            "Epoch 821/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -62.6273 - g_loss: 9271.7309\n",
            "Epoch 822/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -130.7830 - g_loss: 10889.2390\n",
            "Epoch 823/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -160.4421 - g_loss: 11341.0441\n",
            "Epoch 824/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -88.7733 - g_loss: 9208.3810\n",
            "Epoch 825/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -114.9768 - g_loss: 7776.1200\n",
            "Epoch 826/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -10.6630 - g_loss: 7228.3700\n",
            "Epoch 827/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -15.2978 - g_loss: 7766.2666\n",
            "Epoch 828/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 21.5918 - g_loss: 6635.1753\n",
            "Epoch 829/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -101.6197 - g_loss: 5585.3070\n",
            "Epoch 830/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 23.2842 - g_loss: 6213.0037\n",
            "Epoch 831/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 42.3752 - g_loss: 3851.8743\n",
            "Epoch 832/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -43.3774 - g_loss: 2632.1343\n",
            "Epoch 833/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -48.8966 - g_loss: 4901.1691\n",
            "Epoch 834/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -27.7747 - g_loss: 6379.5484\n",
            "Epoch 835/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 37.9650 - g_loss: 6163.9356\n",
            "Epoch 836/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -60.4183 - g_loss: 5122.4904\n",
            "Epoch 837/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -70.7498 - g_loss: 6102.8014\n",
            "Epoch 838/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 8.2636 - g_loss: 5293.8372\n",
            "Epoch 839/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -43.3114 - g_loss: 6211.4350\n",
            "Epoch 840/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 36.9195 - g_loss: 5708.1144\n",
            "Epoch 841/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -78.7784 - g_loss: 5708.6766\n",
            "Epoch 842/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -47.6324 - g_loss: 8247.2557\n",
            "Epoch 843/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 26.4946 - g_loss: 7486.0596\n",
            "Epoch 844/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -121.9573 - g_loss: 7346.8772\n",
            "Epoch 845/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -44.1701 - g_loss: 6387.4345\n",
            "Epoch 846/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -75.8170 - g_loss: 6549.6625\n",
            "Epoch 847/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -4.0878 - g_loss: 6557.9040\n",
            "Epoch 848/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 17.4659 - g_loss: 6099.1297\n",
            "Epoch 849/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -7.7260 - g_loss: 4708.9967\n",
            "Epoch 850/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 8.9596 - g_loss: 4341.1197\n",
            "Epoch 851/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -23.8470 - g_loss: 3914.3324\n",
            "Epoch 852/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -25.8935 - g_loss: 3578.3934\n",
            "Epoch 853/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -0.9869 - g_loss: 3103.3520\n",
            "Epoch 854/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -31.4299 - g_loss: 2855.2202\n",
            "Epoch 855/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -26.6800 - g_loss: 2921.5289\n",
            "Epoch 856/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -47.7709 - g_loss: 2681.7375\n",
            "Epoch 857/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -28.5627 - g_loss: 3931.8261\n",
            "Epoch 858/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -74.2263 - g_loss: 4376.9939\n",
            "Epoch 859/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 66.8258 - g_loss: 3862.7720\n",
            "Epoch 860/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -14.6654 - g_loss: 2707.6275\n",
            "Epoch 861/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -41.6747 - g_loss: 2685.3728\n",
            "Epoch 862/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -24.3104 - g_loss: 3460.7555\n",
            "Epoch 863/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -1.1786 - g_loss: 4094.0526\n",
            "Epoch 864/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -30.1972 - g_loss: 4458.6762\n",
            "Epoch 865/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 17.1113 - g_loss: 3657.4657\n",
            "Epoch 866/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -14.8935 - g_loss: 3710.5462\n",
            "Epoch 867/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -56.6954 - g_loss: 4064.5435\n",
            "Epoch 868/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -48.7878 - g_loss: 5108.2888\n",
            "Epoch 869/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -52.6207 - g_loss: 5800.7759\n",
            "Epoch 870/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -19.1970 - g_loss: 5151.7530\n",
            "Epoch 871/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -20.6295 - g_loss: 4110.2550\n",
            "Epoch 872/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -25.8110 - g_loss: 4009.2266\n",
            "Epoch 873/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 5.3029 - g_loss: 5004.4874\n",
            "Epoch 874/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -40.9199 - g_loss: 5490.5690\n",
            "Epoch 875/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 35.7910 - g_loss: 5134.4970\n",
            "Epoch 876/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -17.0082 - g_loss: 5419.3405\n",
            "Epoch 877/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -55.9683 - g_loss: 7666.8441\n",
            "Epoch 878/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -38.6717 - g_loss: 8391.1726\n",
            "Epoch 879/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -27.6087 - g_loss: 8764.3045\n",
            "Epoch 880/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 113.3190 - g_loss: 8496.8308\n",
            "Epoch 881/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 3.8858 - g_loss: 6804.4364\n",
            "Epoch 882/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -129.1727 - g_loss: 7681.6214\n",
            "Epoch 883/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -71.2776 - g_loss: 7453.2453\n",
            "Epoch 884/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -34.0241 - g_loss: 6369.7498\n",
            "Epoch 885/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 9.6375 - g_loss: 4341.6247\n",
            "Epoch 886/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 30.1225 - g_loss: 3380.6947\n",
            "Epoch 887/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -42.7755 - g_loss: 2681.6256\n",
            "Epoch 888/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -21.8413 - g_loss: 2423.3611\n",
            "Epoch 889/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -8.8376 - g_loss: 2017.9921\n",
            "Epoch 890/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -65.8065 - g_loss: 3274.2762\n",
            "Epoch 891/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 4.0409 - g_loss: 2683.0083\n",
            "Epoch 892/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -32.3955 - g_loss: 3199.7916\n",
            "Epoch 893/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 42.3185 - g_loss: 4952.7405\n",
            "Epoch 894/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 101.0802 - g_loss: 5167.6452\n",
            "Epoch 895/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -59.0068 - g_loss: 5105.8422\n",
            "Epoch 896/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 8.6197 - g_loss: 6504.4484\n",
            "Epoch 897/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -16.6213 - g_loss: 5359.6922\n",
            "Epoch 898/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -77.7038 - g_loss: 4629.8517\n",
            "Epoch 899/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -33.6215 - g_loss: 4755.9826\n",
            "Epoch 900/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -62.6616 - g_loss: 4818.7514\n",
            "Epoch 901/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -39.7620 - g_loss: 5564.7141\n",
            "Epoch 902/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 32.3133 - g_loss: 4179.0507\n",
            "Epoch 903/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 30.7022 - g_loss: 4339.9939\n",
            "Epoch 904/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -22.7507 - g_loss: 4305.3368\n",
            "Epoch 905/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -73.2290 - g_loss: 4714.9780\n",
            "Epoch 906/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -42.4578 - g_loss: 3994.1882\n",
            "Epoch 907/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -47.5835 - g_loss: 4751.7145\n",
            "Epoch 908/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -52.3029 - g_loss: 5008.3914\n",
            "Epoch 909/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 55.7967 - g_loss: 5651.2919\n",
            "Epoch 910/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -118.2086 - g_loss: 6648.5675\n",
            "Epoch 911/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 17.8843 - g_loss: 6057.5376\n",
            "Epoch 912/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -6.7365 - g_loss: 4764.3155\n",
            "Epoch 913/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 40.8001 - g_loss: 4282.1979\n",
            "Epoch 914/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 9.1223 - g_loss: 5197.7146\n",
            "Epoch 915/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 133.7647 - g_loss: 6546.2349\n",
            "Epoch 916/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -14.4990 - g_loss: 5622.3241\n",
            "Epoch 917/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -30.6278 - g_loss: 5221.2563\n",
            "Epoch 918/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -25.7578 - g_loss: 6171.9785\n",
            "Epoch 919/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 6.4924 - g_loss: 7607.9865\n",
            "Epoch 920/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -16.5463 - g_loss: 7987.9979\n",
            "Epoch 921/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 6.8213 - g_loss: 8101.2328\n",
            "Epoch 922/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -76.2724 - g_loss: 7910.9466\n",
            "Epoch 923/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 21.4732 - g_loss: 6111.5588\n",
            "Epoch 924/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -19.1900 - g_loss: 5877.7829\n",
            "Epoch 925/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -15.3882 - g_loss: 7212.3273\n",
            "Epoch 926/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -111.4836 - g_loss: 6734.2991\n",
            "Epoch 927/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 4.6997 - g_loss: 5195.1122\n",
            "Epoch 928/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -46.1961 - g_loss: 5465.8820\n",
            "Epoch 929/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -78.2331 - g_loss: 4401.6503\n",
            "Epoch 930/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 54.0902 - g_loss: 4215.1579\n",
            "Epoch 931/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -9.2509 - g_loss: 4280.2751\n",
            "Epoch 932/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -23.9348 - g_loss: 4291.1128\n",
            "Epoch 933/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 25.7646 - g_loss: 3745.2878\n",
            "Epoch 934/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -4.9722 - g_loss: 4088.3394\n",
            "Epoch 935/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -6.1527 - g_loss: 6328.7790\n",
            "Epoch 936/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -125.3187 - g_loss: 7496.2360\n",
            "Epoch 937/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 47.7511 - g_loss: 9063.5477\n",
            "Epoch 938/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -65.7550 - g_loss: 6879.9992\n",
            "Epoch 939/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -8.8700 - g_loss: 6908.6279\n",
            "Epoch 940/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -15.7170 - g_loss: 6230.5963\n",
            "Epoch 941/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -102.7568 - g_loss: 6575.2874\n",
            "Epoch 942/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -96.4446 - g_loss: 8501.1503\n",
            "Epoch 943/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -96.7516 - g_loss: 8095.7833\n",
            "Epoch 944/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -67.9000 - g_loss: 7971.4624\n",
            "Epoch 945/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 15.2362 - g_loss: 8416.2730\n",
            "Epoch 946/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -48.6043 - g_loss: 8252.0448\n",
            "Epoch 947/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -40.7171 - g_loss: 9199.3092\n",
            "Epoch 948/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -30.9621 - g_loss: 8733.5854\n",
            "Epoch 949/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 134.5589 - g_loss: 9536.9954\n",
            "Epoch 950/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -134.6669 - g_loss: 8774.5031\n",
            "Epoch 951/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 8.5071 - g_loss: 9767.0050\n",
            "Epoch 952/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 94.3307 - g_loss: 7935.2211\n",
            "Epoch 953/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -82.0352 - g_loss: 8095.2046\n",
            "Epoch 954/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 72.0183 - g_loss: 7029.2107\n",
            "Epoch 955/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 50.1800 - g_loss: 7552.4285\n",
            "Epoch 956/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 140.9434 - g_loss: 6969.5100\n",
            "Epoch 957/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -112.4154 - g_loss: 7374.8314\n",
            "Epoch 958/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 10.0391 - g_loss: 6554.1581\n",
            "Epoch 959/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -6.1006 - g_loss: 6258.8383\n",
            "Epoch 960/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 44.6813 - g_loss: 5215.3778\n",
            "Epoch 961/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 18.2475 - g_loss: 6290.0833\n",
            "Epoch 962/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -9.9481 - g_loss: 7585.5748\n",
            "Epoch 963/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -59.5597 - g_loss: 9151.5825\n",
            "Epoch 964/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 11.5378 - g_loss: 10830.2918\n",
            "Epoch 965/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -44.8949 - g_loss: 9659.5277\n",
            "Epoch 966/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -18.8313 - g_loss: 9161.8554\n",
            "Epoch 967/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 48.3197 - g_loss: 8737.7889\n",
            "Epoch 968/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 16.1982 - g_loss: 8753.3674\n",
            "Epoch 969/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 6.9966 - g_loss: 9128.7029\n",
            "Epoch 970/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 42.0216 - g_loss: 9148.8809\n",
            "Epoch 971/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -10.7427 - g_loss: 8058.4828\n",
            "Epoch 972/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -84.9613 - g_loss: 7687.4647\n",
            "Epoch 973/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 13.2303 - g_loss: 8000.0102\n",
            "Epoch 974/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -34.6514 - g_loss: 7378.8380\n",
            "Epoch 975/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -10.3047 - g_loss: 6301.7865\n",
            "Epoch 976/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -59.0147 - g_loss: 4570.1049\n",
            "Epoch 977/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -21.5335 - g_loss: 6485.3671\n",
            "Epoch 978/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 45.4286 - g_loss: 7691.5880\n",
            "Epoch 979/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -135.5642 - g_loss: 7637.3799\n",
            "Epoch 980/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -88.8508 - g_loss: 7020.7701\n",
            "Epoch 981/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -190.3781 - g_loss: 6087.5458\n",
            "Epoch 982/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -175.5492 - g_loss: 6500.2167\n",
            "Epoch 983/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -205.3839 - g_loss: 3590.3088\n",
            "Epoch 984/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -53.3117 - g_loss: 5162.6105\n",
            "Epoch 985/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -239.3159 - g_loss: 4380.8160\n",
            "Epoch 986/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 180.2858 - g_loss: 1993.7865\n",
            "Epoch 987/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 220.6310 - g_loss: 2256.5304\n",
            "Epoch 988/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 90.9462 - g_loss: 2545.9464\n",
            "Epoch 989/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -37.1952 - g_loss: 2725.9942\n",
            "Epoch 990/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 126.3321 - g_loss: 3216.4752\n",
            "Epoch 991/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -330.5176 - g_loss: 1448.9502\n",
            "Epoch 992/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 35.7614 - g_loss: 1504.9722\n",
            "Epoch 993/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -654.7386 - g_loss: 1527.5786\n",
            "Epoch 994/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -29.6712 - g_loss: 2974.6982\n",
            "Epoch 995/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 45.8364 - g_loss: 4810.1013\n",
            "Epoch 996/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 104.5973 - g_loss: 4680.8522\n",
            "Epoch 997/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -271.0094 - g_loss: 5689.4662\n",
            "Epoch 998/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -290.0568 - g_loss: 6919.6644\n",
            "Epoch 999/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 393.6857 - g_loss: 7167.7998\n",
            "Epoch 1000/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 251.7740 - g_loss: 7116.0907\n",
            "Epoch 1001/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -257.6242 - g_loss: 6307.3439\n",
            "Epoch 1002/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -58.0833 - g_loss: 6289.1647\n",
            "Epoch 1003/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -168.3935 - g_loss: 5150.8885\n",
            "Epoch 1004/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -201.2215 - g_loss: 3891.3763\n",
            "Epoch 1005/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 133.6027 - g_loss: 3534.3636\n",
            "Epoch 1006/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -185.5375 - g_loss: 2331.3084\n",
            "Epoch 1007/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 489.7164 - g_loss: 2114.6515\n",
            "Epoch 1008/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 31.0077 - g_loss: 1829.9336\n",
            "Epoch 1009/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -115.1105 - g_loss: 1762.9080\n",
            "Epoch 1010/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 40.3803 - g_loss: 1507.7060\n",
            "Epoch 1011/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -74.1329 - g_loss: 456.5145\n",
            "Epoch 1012/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 83.8868 - g_loss: 983.2112\n",
            "Epoch 1013/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -86.8600 - g_loss: 2262.3270\n",
            "Epoch 1014/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -24.0141 - g_loss: 4258.8186\n",
            "Epoch 1015/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -20.7658 - g_loss: 4489.6560\n",
            "Epoch 1016/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -487.8978 - g_loss: 5319.1145\n",
            "Epoch 1017/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 139.8376 - g_loss: 4835.4072\n",
            "Epoch 1018/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 141.5904 - g_loss: 2622.1852\n",
            "Epoch 1019/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -171.9414 - g_loss: 1352.1683\n",
            "Epoch 1020/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -43.4843 - g_loss: 1683.2607\n",
            "Epoch 1021/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 83.4070 - g_loss: 2777.2004\n",
            "Epoch 1022/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 122.9744 - g_loss: 2539.4082\n",
            "Epoch 1023/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 37.4200 - g_loss: 3433.6981\n",
            "Epoch 1024/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -76.0441 - g_loss: 3345.7987\n",
            "Epoch 1025/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 66.2893 - g_loss: 2476.9512\n",
            "Epoch 1026/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -288.5939 - g_loss: 1375.2714\n",
            "Epoch 1027/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -391.5699 - g_loss: -52.2810\n",
            "Epoch 1028/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 271.1374 - g_loss: 858.1267\n",
            "Epoch 1029/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 61.7844 - g_loss: 1648.8538\n",
            "Epoch 1030/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 48.1731 - g_loss: 2862.1093\n",
            "Epoch 1031/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 25.2624 - g_loss: 1971.4273\n",
            "Epoch 1032/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -141.6247 - g_loss: 1212.3166\n",
            "Epoch 1033/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 116.3699 - g_loss: 2138.0795\n",
            "Epoch 1034/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -7.6596 - g_loss: 2139.9640\n",
            "Epoch 1035/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -51.7992 - g_loss: 2223.5878\n",
            "Epoch 1036/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 2.9346 - g_loss: 2816.1078\n",
            "Epoch 1037/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -66.7073 - g_loss: 1469.3119\n",
            "Epoch 1038/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -167.1791 - g_loss: 543.0722\n",
            "Epoch 1039/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -100.0749 - g_loss: 1145.6781\n",
            "Epoch 1040/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -122.9685 - g_loss: 2693.2639\n",
            "Epoch 1041/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -90.0766 - g_loss: 2665.1516\n",
            "Epoch 1042/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -47.7197 - g_loss: 3536.8822\n",
            "Epoch 1043/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -166.4939 - g_loss: 3190.7052\n",
            "Epoch 1044/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 252.5769 - g_loss: 3917.6298\n",
            "Epoch 1045/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 155.9593 - g_loss: 4035.7941\n",
            "Epoch 1046/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -522.2989 - g_loss: 2521.5358\n",
            "Epoch 1047/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -403.7672 - g_loss: 3413.1267\n",
            "Epoch 1048/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -188.4321 - g_loss: 2776.8840\n",
            "Epoch 1049/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 85.9816 - g_loss: -1922.3923\n",
            "Epoch 1050/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 261.4357 - g_loss: -2388.7668\n",
            "Epoch 1051/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 418.6661 - g_loss: -407.2447\n",
            "Epoch 1052/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -30.9873 - g_loss: 1581.6614\n",
            "Epoch 1053/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -472.2051 - g_loss: 4931.7168\n",
            "Epoch 1054/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -71.5092 - g_loss: 4362.0874\n",
            "Epoch 1055/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -381.3495 - g_loss: 4956.7439\n",
            "Epoch 1056/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -10.9293 - g_loss: 6757.8124\n",
            "Epoch 1057/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 129.4378 - g_loss: 4369.1176\n",
            "Epoch 1058/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 502.1391 - g_loss: 2690.9520\n",
            "Epoch 1059/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 14.2225 - g_loss: 1293.6733\n",
            "Epoch 1060/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 136.8104 - g_loss: 2451.6759\n",
            "Epoch 1061/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -278.8203 - g_loss: 1412.9303\n",
            "Epoch 1062/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 0.5232 - g_loss: 1979.5961\n",
            "Epoch 1063/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -5.3872 - g_loss: 3384.5656\n",
            "Epoch 1064/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 32.9986 - g_loss: 2474.2256\n",
            "Epoch 1065/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -168.5092 - g_loss: 1798.2340\n",
            "Epoch 1066/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 289.6093 - g_loss: 1327.2265\n",
            "Epoch 1067/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 48.8404 - g_loss: 3176.4787\n",
            "Epoch 1068/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -118.3783 - g_loss: 6775.0495\n",
            "Epoch 1069/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -212.8106 - g_loss: 9066.9183\n",
            "Epoch 1070/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 128.7415 - g_loss: 10324.0243\n",
            "Epoch 1071/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -485.7458 - g_loss: 9371.9180\n",
            "Epoch 1072/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -213.4135 - g_loss: 9467.5307\n",
            "Epoch 1073/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 248.5465 - g_loss: 8722.7889\n",
            "Epoch 1074/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -166.7175 - g_loss: 8461.8335\n",
            "Epoch 1075/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 94.8880 - g_loss: 8019.2949\n",
            "Epoch 1076/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 124.5095 - g_loss: 5190.2987\n",
            "Epoch 1077/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 216.9424 - g_loss: 3564.1207\n",
            "Epoch 1078/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -89.5341 - g_loss: 3531.0360\n",
            "Epoch 1079/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 70.4735 - g_loss: 2915.7362\n",
            "Epoch 1080/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 138.2400 - g_loss: 2745.1520\n",
            "Epoch 1081/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -60.9240 - g_loss: 1774.2047\n",
            "Epoch 1082/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 208.9059 - g_loss: 2200.1485\n",
            "Epoch 1083/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 25.5935 - g_loss: 2185.3198\n",
            "Epoch 1084/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 65.0831 - g_loss: 2111.6636\n",
            "Epoch 1085/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 173.6980 - g_loss: 184.6786\n",
            "Epoch 1086/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -106.7632 - g_loss: 981.4624\n",
            "Epoch 1087/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -38.4655 - g_loss: 2320.2029\n",
            "Epoch 1088/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -10.5189 - g_loss: 2554.6138\n",
            "Epoch 1089/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 40.0082 - g_loss: 4460.0062\n",
            "Epoch 1090/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 233.2403 - g_loss: 4489.0039\n",
            "Epoch 1091/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -216.1735 - g_loss: 3995.7287\n",
            "Epoch 1092/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -77.0430 - g_loss: 5676.5779\n",
            "Epoch 1093/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 137.5046 - g_loss: 6448.4606\n",
            "Epoch 1094/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 128.5071 - g_loss: 6377.1084\n",
            "Epoch 1095/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -170.6385 - g_loss: 7849.7029\n",
            "Epoch 1096/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -181.4934 - g_loss: 9128.6377\n",
            "Epoch 1097/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -150.9513 - g_loss: 8932.8422\n",
            "Epoch 1098/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -19.5285 - g_loss: 9751.9282\n",
            "Epoch 1099/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -189.6623 - g_loss: 10761.2853\n",
            "Epoch 1100/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 56.3943 - g_loss: 9255.2334\n",
            "Epoch 1101/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -180.5903 - g_loss: 8344.5705\n",
            "Epoch 1102/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 7.4483 - g_loss: 6745.9838\n",
            "Epoch 1103/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 21.6655 - g_loss: 4814.7335\n",
            "Epoch 1104/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 147.9822 - g_loss: 4593.6410\n",
            "Epoch 1105/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -52.8667 - g_loss: 3653.6877\n",
            "Epoch 1106/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -77.1145 - g_loss: 3401.8756\n",
            "Epoch 1107/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -130.2977 - g_loss: 2062.1065\n",
            "Epoch 1108/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -86.3102 - g_loss: 2075.5606\n",
            "Epoch 1109/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 7.7075 - g_loss: 1837.8495\n",
            "Epoch 1110/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -179.2768 - g_loss: 1848.9771\n",
            "Epoch 1111/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 5.7351 - g_loss: 2436.4430\n",
            "Epoch 1112/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 52.3029 - g_loss: 3992.6118\n",
            "Epoch 1113/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -41.2527 - g_loss: 4090.7512\n",
            "Epoch 1114/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 83.7155 - g_loss: 4437.7574\n",
            "Epoch 1115/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 65.8467 - g_loss: 4701.2288\n",
            "Epoch 1116/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 118.1135 - g_loss: 4447.6030\n",
            "Epoch 1117/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -21.5116 - g_loss: 3939.7938\n",
            "Epoch 1118/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -16.4442 - g_loss: 2594.5376\n",
            "Epoch 1119/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -69.9400 - g_loss: 2869.8266\n",
            "Epoch 1120/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -95.2531 - g_loss: 3837.1006\n",
            "Epoch 1121/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -258.7392 - g_loss: 3431.4149\n",
            "Epoch 1122/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -156.1636 - g_loss: 3292.3869\n",
            "Epoch 1123/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -176.1420 - g_loss: 1593.7484\n",
            "Epoch 1124/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -53.9145 - g_loss: 2318.9993\n",
            "Epoch 1125/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -161.3425 - g_loss: 2973.3664\n",
            "Epoch 1126/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -178.3834 - g_loss: 3317.6812\n",
            "Epoch 1127/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -63.1881 - g_loss: 2378.2848\n",
            "Epoch 1128/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -137.1232 - g_loss: 1948.3413\n",
            "Epoch 1129/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 228.6597 - g_loss: 2253.9455\n",
            "Epoch 1130/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -178.4475 - g_loss: 1518.2665\n",
            "Epoch 1131/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -80.0175 - g_loss: 934.9880\n",
            "Epoch 1132/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -37.2708 - g_loss: 901.8733\n",
            "Epoch 1133/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -53.6587 - g_loss: 3073.8890\n",
            "Epoch 1134/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 86.3118 - g_loss: 3932.2682\n",
            "Epoch 1135/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 12.0276 - g_loss: 3590.2825\n",
            "Epoch 1136/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -37.8401 - g_loss: 4449.4084\n",
            "Epoch 1137/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -72.4096 - g_loss: 4909.7351\n",
            "Epoch 1138/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 245.5877 - g_loss: 5080.2725\n",
            "Epoch 1139/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -154.3514 - g_loss: 4248.3285\n",
            "Epoch 1140/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 87.8198 - g_loss: 4577.4525\n",
            "Epoch 1141/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -165.2147 - g_loss: 5778.3963\n",
            "Epoch 1142/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -277.1079 - g_loss: 4981.2379\n",
            "Epoch 1143/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -241.7345 - g_loss: 4040.0024\n",
            "Epoch 1144/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -97.6856 - g_loss: 4782.2328\n",
            "Epoch 1145/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 73.5838 - g_loss: 6033.7235\n",
            "Epoch 1146/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -249.9512 - g_loss: 7167.9774\n",
            "Epoch 1147/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 140.4656 - g_loss: 4805.4924\n",
            "Epoch 1148/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 180.8634 - g_loss: 4079.3273\n",
            "Epoch 1149/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -240.8302 - g_loss: 4051.1886\n",
            "Epoch 1150/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 120.6329 - g_loss: 4307.4395\n",
            "Epoch 1151/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -168.0981 - g_loss: 5148.6014\n",
            "Epoch 1152/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -90.0069 - g_loss: 6015.1813\n",
            "Epoch 1153/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 99.5340 - g_loss: 4356.6767\n",
            "Epoch 1154/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -43.6889 - g_loss: 3943.9869\n",
            "Epoch 1155/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -193.1951 - g_loss: 4428.3835\n",
            "Epoch 1156/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -91.3696 - g_loss: 4434.2657\n",
            "Epoch 1157/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 18.8961 - g_loss: 3965.4260\n",
            "Epoch 1158/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 25.8682 - g_loss: 4452.5276\n",
            "Epoch 1159/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 94.7835 - g_loss: 2858.3813\n",
            "Epoch 1160/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 182.0628 - g_loss: 3027.2476\n",
            "Epoch 1161/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 62.1977 - g_loss: 3034.1643\n",
            "Epoch 1162/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 91.0716 - g_loss: 1822.5759\n",
            "Epoch 1163/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 151.2090 - g_loss: 1752.5589\n",
            "Epoch 1164/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -11.8332 - g_loss: 2272.0283\n",
            "Epoch 1165/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 140.4258 - g_loss: 2477.6827\n",
            "Epoch 1166/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -64.1938 - g_loss: 2045.3735\n",
            "Epoch 1167/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -323.3996 - g_loss: 2191.4031\n",
            "Epoch 1168/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -461.2701 - g_loss: 2998.6509\n",
            "Epoch 1169/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 237.8223 - g_loss: 3006.9618\n",
            "Epoch 1170/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 405.1150 - g_loss: 2113.9037\n",
            "Epoch 1171/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -300.4686 - g_loss: 355.9091\n",
            "Epoch 1172/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -345.4612 - g_loss: 817.5576\n",
            "Epoch 1173/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -186.2133 - g_loss: 1612.9798\n",
            "Epoch 1174/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -130.9460 - g_loss: 1792.3959\n",
            "Epoch 1175/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -22.2179 - g_loss: 2434.3374\n",
            "Epoch 1176/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -174.9025 - g_loss: 2112.8662\n",
            "Epoch 1177/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 272.1489 - g_loss: 1019.6363\n",
            "Epoch 1178/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 171.7481 - g_loss: -1582.1678\n",
            "Epoch 1179/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 9.9338 - g_loss: -1752.8220\n",
            "Epoch 1180/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -247.4136 - g_loss: -2746.9413\n",
            "Epoch 1181/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 78.8426 - g_loss: -4918.8368\n",
            "Epoch 1182/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -16.7867 - g_loss: -2565.9295\n",
            "Epoch 1183/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -12.4967 - g_loss: -2151.8593\n",
            "Epoch 1184/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -64.4314 - g_loss: -2241.8637\n",
            "Epoch 1185/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -483.7809 - g_loss: -1978.3327\n",
            "Epoch 1186/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 725.2755 - g_loss: -2341.3720\n",
            "Epoch 1187/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 247.8264 - g_loss: -3258.3649\n",
            "Epoch 1188/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 208.3327 - g_loss: -3123.7989\n",
            "Epoch 1189/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -248.7606 - g_loss: -4467.0788\n",
            "Epoch 1190/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 267.2440 - g_loss: -2861.2067\n",
            "Epoch 1191/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -37.5503 - g_loss: -2112.0529\n",
            "Epoch 1192/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -360.7732 - g_loss: -495.7704\n",
            "Epoch 1193/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -315.0367 - g_loss: 583.4921\n",
            "Epoch 1194/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -418.2926 - g_loss: -245.1380\n",
            "Epoch 1195/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -65.2945 - g_loss: -891.9944\n",
            "Epoch 1196/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -337.4158 - g_loss: -1044.2391\n",
            "Epoch 1197/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -562.4126 - g_loss: -3904.0633\n",
            "Epoch 1198/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 147.4344 - g_loss: -5087.1482\n",
            "Epoch 1199/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 547.8815 - g_loss: -3929.5777\n",
            "Epoch 1200/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -34.9209 - g_loss: -3551.8247\n",
            "Epoch 1201/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 70.9389 - g_loss: -2434.0667\n",
            "Epoch 1202/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 466.4381 - g_loss: -3054.4019\n",
            "Epoch 1203/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -100.9111 - g_loss: -3468.9336\n",
            "Epoch 1204/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -398.4796 - g_loss: -5442.2636\n",
            "Epoch 1205/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -256.9546 - g_loss: -5473.9563\n",
            "Epoch 1206/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -293.9006 - g_loss: -3166.4225\n",
            "Epoch 1207/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 179.7334 - g_loss: -2573.4063\n",
            "Epoch 1208/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -134.9799 - g_loss: -2362.8811\n",
            "Epoch 1209/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 4.4147 - g_loss: -2189.9333\n",
            "Epoch 1210/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 11.2688 - g_loss: -1691.2605\n",
            "Epoch 1211/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 99.6222 - g_loss: -2858.3923\n",
            "Epoch 1212/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 42.4392 - g_loss: -2929.7137\n",
            "Epoch 1213/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -552.7585 - g_loss: -3449.6134\n",
            "Epoch 1214/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 107.3017 - g_loss: -3046.8787\n",
            "Epoch 1215/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 188.8124 - g_loss: -3981.6661\n",
            "Epoch 1216/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 205.1283 - g_loss: -3072.2332\n",
            "Epoch 1217/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 410.2012 - g_loss: -907.2294\n",
            "Epoch 1218/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -206.8752 - g_loss: 1576.8362\n",
            "Epoch 1219/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 65.7574 - g_loss: 1836.7690\n",
            "Epoch 1220/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -20.5511 - g_loss: 1891.9217\n",
            "Epoch 1221/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 429.7884 - g_loss: 848.0951\n",
            "Epoch 1222/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -386.3507 - g_loss: 780.5694\n",
            "Epoch 1223/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 854.1771 - g_loss: -173.3604\n",
            "Epoch 1224/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -182.5347 - g_loss: -1271.9600\n",
            "Epoch 1225/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 988.4167 - g_loss: -2474.5051\n",
            "Epoch 1226/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 252.2242 - g_loss: -1312.7853\n",
            "Epoch 1227/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 558.9133 - g_loss: -1277.2651\n",
            "Epoch 1228/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -175.5920 - g_loss: -2004.8063\n",
            "Epoch 1229/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 84.4051 - g_loss: -3552.4222\n",
            "Epoch 1230/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -331.7764 - g_loss: -1769.8258\n",
            "Epoch 1231/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 70.2566 - g_loss: -1244.5282\n",
            "Epoch 1232/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -232.7597 - g_loss: -1626.3653\n",
            "Epoch 1233/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -87.0933 - g_loss: -4855.5355\n",
            "Epoch 1234/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 106.8494 - g_loss: -3483.7610\n",
            "Epoch 1235/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -505.4552 - g_loss: -5337.9174\n",
            "Epoch 1236/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 381.7967 - g_loss: -6860.3538\n",
            "Epoch 1237/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 12.7652 - g_loss: -4334.8007\n",
            "Epoch 1238/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 132.6563 - g_loss: -4988.6178\n",
            "Epoch 1239/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -491.6037 - g_loss: -4446.5420\n",
            "Epoch 1240/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 116.4761 - g_loss: -2545.5750\n",
            "Epoch 1241/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -239.5468 - g_loss: -4034.4069\n",
            "Epoch 1242/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -306.6609 - g_loss: -4351.6811\n",
            "Epoch 1243/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 238.1017 - g_loss: -1477.2495\n",
            "Epoch 1244/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -33.7694 - g_loss: 451.8897\n",
            "Epoch 1245/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 210.8765 - g_loss: 3899.1167\n",
            "Epoch 1246/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -485.8704 - g_loss: 5378.9422\n",
            "Epoch 1247/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -143.6372 - g_loss: 4014.8350\n",
            "Epoch 1248/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -791.9183 - g_loss: 4447.5402\n",
            "Epoch 1249/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 129.0317 - g_loss: 5494.0718\n",
            "Epoch 1250/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 1196.3128 - g_loss: 4399.1465\n",
            "Epoch 1251/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -490.7146 - g_loss: 2880.4690\n",
            "Epoch 1252/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 247.2400 - g_loss: -1.8676\n",
            "Epoch 1253/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 1515.3427 - g_loss: 2171.5994\n",
            "Epoch 1254/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 55.8878 - g_loss: 3234.3867\n",
            "Epoch 1255/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 1048.9530 - g_loss: 3242.5146\n",
            "Epoch 1256/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -4.1384 - g_loss: -841.9439\n",
            "Epoch 1257/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -368.9077 - g_loss: 18.8176\n",
            "Epoch 1258/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -73.3523 - g_loss: 1081.7607\n",
            "Epoch 1259/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -230.9400 - g_loss: -2376.7323\n",
            "Epoch 1260/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 921.8980 - g_loss: -1569.5005\n",
            "Epoch 1261/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -279.8074 - g_loss: 1206.5679\n",
            "Epoch 1262/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 823.7848 - g_loss: 1174.9165\n",
            "Epoch 1263/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 551.1511 - g_loss: 1107.6111\n",
            "Epoch 1264/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -232.4846 - g_loss: 2419.0741\n",
            "Epoch 1265/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -71.2888 - g_loss: 2890.6813\n",
            "Epoch 1266/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 15.0058 - g_loss: 2128.5200\n",
            "Epoch 1267/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -101.7035 - g_loss: 4471.0873\n",
            "Epoch 1268/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 251.2668 - g_loss: 5165.7044\n",
            "Epoch 1269/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -96.4512 - g_loss: 5130.5105\n",
            "Epoch 1270/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 509.0292 - g_loss: 4605.3267\n",
            "Epoch 1271/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 213.3588 - g_loss: 3602.0998\n",
            "Epoch 1272/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -55.4260 - g_loss: 2575.0908\n",
            "Epoch 1273/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 81.7948 - g_loss: 3262.9949\n",
            "Epoch 1274/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 152.8906 - g_loss: 2944.5147\n",
            "Epoch 1275/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -46.8527 - g_loss: 3582.8205\n",
            "Epoch 1276/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -142.5358 - g_loss: 4587.5229\n",
            "Epoch 1277/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -182.9823 - g_loss: 3638.3532\n",
            "Epoch 1278/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -486.4495 - g_loss: 4871.9863\n",
            "Epoch 1279/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -286.9565 - g_loss: 6877.9177\n",
            "Epoch 1280/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 64.7058 - g_loss: 6395.7758\n",
            "Epoch 1281/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -61.5298 - g_loss: 5283.6946\n",
            "Epoch 1282/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 588.0271 - g_loss: 4111.6692\n",
            "Epoch 1283/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -189.9170 - g_loss: 4896.9942\n",
            "Epoch 1284/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -94.9258 - g_loss: 5854.7098\n",
            "Epoch 1285/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -575.5283 - g_loss: 5507.0849\n",
            "Epoch 1286/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 668.7014 - g_loss: 5886.9745\n",
            "Epoch 1287/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 169.7262 - g_loss: 6531.7767\n",
            "Epoch 1288/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -41.3177 - g_loss: 7739.4556\n",
            "Epoch 1289/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -354.6448 - g_loss: 7891.9545\n",
            "Epoch 1290/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 203.7596 - g_loss: 7556.7477\n",
            "Epoch 1291/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 52.7788 - g_loss: 6273.7879\n",
            "Epoch 1292/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 82.9795 - g_loss: 6214.8506\n",
            "Epoch 1293/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 29.4443 - g_loss: 4561.3387\n",
            "Epoch 1294/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -209.9909 - g_loss: 7381.8586\n",
            "Epoch 1295/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -10.8403 - g_loss: 8384.8579\n",
            "Epoch 1296/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 1.5975 - g_loss: 8196.8954\n",
            "Epoch 1297/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 183.1561 - g_loss: 8764.1664\n",
            "Epoch 1298/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -411.9377 - g_loss: 9278.8344\n",
            "Epoch 1299/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -167.3912 - g_loss: 7202.0727\n",
            "Epoch 1300/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 285.6837 - g_loss: 7674.5472\n",
            "Epoch 1301/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -150.1407 - g_loss: 6746.1223\n",
            "Epoch 1302/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 353.3711 - g_loss: 5292.2516\n",
            "Epoch 1303/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -142.3359 - g_loss: 5947.8793\n",
            "Epoch 1304/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 188.9281 - g_loss: 5003.0479\n",
            "Epoch 1305/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -232.1222 - g_loss: 4236.9116\n",
            "Epoch 1306/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 312.1341 - g_loss: 2938.1809\n",
            "Epoch 1307/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 7.4261 - g_loss: 2569.0180\n",
            "Epoch 1308/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -171.4125 - g_loss: 2607.6820\n",
            "Epoch 1309/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 151.2182 - g_loss: 2630.8617\n",
            "Epoch 1310/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -79.5352 - g_loss: 3677.2640\n",
            "Epoch 1311/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -37.8385 - g_loss: 3193.6486\n",
            "Epoch 1312/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -25.2250 - g_loss: 2800.1621\n",
            "Epoch 1313/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -17.4447 - g_loss: 2346.7046\n",
            "Epoch 1314/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 84.5447 - g_loss: 1603.8164\n",
            "Epoch 1315/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -39.3141 - g_loss: 1622.0026\n",
            "Epoch 1316/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -15.7792 - g_loss: 1656.7316\n",
            "Epoch 1317/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 4.9524 - g_loss: 1861.0732\n",
            "Epoch 1318/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -28.2555 - g_loss: 1990.1559\n",
            "Epoch 1319/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 5.1854 - g_loss: 2317.2751\n",
            "Epoch 1320/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 54.1603 - g_loss: 2957.5518\n",
            "Epoch 1321/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -37.6052 - g_loss: 2529.1732\n",
            "Epoch 1322/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -54.9547 - g_loss: 2948.1620\n",
            "Epoch 1323/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -52.1833 - g_loss: 4738.7886\n",
            "Epoch 1324/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -83.0161 - g_loss: 7069.3967\n",
            "Epoch 1325/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 15.3385 - g_loss: 7309.2331\n",
            "Epoch 1326/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 253.8413 - g_loss: 7040.4321\n",
            "Epoch 1327/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -100.2987 - g_loss: 6764.9658\n",
            "Epoch 1328/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 27.8563 - g_loss: 6343.4187\n",
            "Epoch 1329/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -41.6620 - g_loss: 7258.5030\n",
            "Epoch 1330/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 72.5080 - g_loss: 7065.7438\n",
            "Epoch 1331/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -76.4703 - g_loss: 6364.5675\n",
            "Epoch 1332/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -4.0033 - g_loss: 5738.9773\n",
            "Epoch 1333/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 185.2076 - g_loss: 5810.3294\n",
            "Epoch 1334/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -164.3683 - g_loss: 6682.6613\n",
            "Epoch 1335/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -190.7755 - g_loss: 6410.0317\n",
            "Epoch 1336/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 7.5773 - g_loss: 6909.5407\n",
            "Epoch 1337/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 136.4844 - g_loss: 6594.8616\n",
            "Epoch 1338/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -7.2649 - g_loss: 6548.2686\n",
            "Epoch 1339/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 39.8042 - g_loss: 6305.4384\n",
            "Epoch 1340/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 248.5282 - g_loss: 5441.8285\n",
            "Epoch 1341/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -38.8624 - g_loss: 4201.5108\n",
            "Epoch 1342/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 214.6957 - g_loss: 4780.8032\n",
            "Epoch 1343/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -276.4408 - g_loss: 3893.0234\n",
            "Epoch 1344/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -86.4601 - g_loss: 4470.5002\n",
            "Epoch 1345/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 15.6714 - g_loss: 4044.1792\n",
            "Epoch 1346/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -269.4890 - g_loss: 4151.6061\n",
            "Epoch 1347/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -86.0947 - g_loss: 2904.4441\n",
            "Epoch 1348/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 97.2355 - g_loss: 2028.4723\n",
            "Epoch 1349/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -11.8168 - g_loss: 2251.5877\n",
            "Epoch 1350/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -87.5992 - g_loss: 3327.3415\n",
            "Epoch 1351/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -122.2780 - g_loss: 2672.1253\n",
            "Epoch 1352/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 59.1972 - g_loss: 2677.1073\n",
            "Epoch 1353/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -37.2892 - g_loss: 2170.3910\n",
            "Epoch 1354/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 40.9891 - g_loss: 1793.4232\n",
            "Epoch 1355/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -22.5366 - g_loss: 1732.8720\n",
            "Epoch 1356/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -13.1787 - g_loss: 1586.4542\n",
            "Epoch 1357/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 63.7185 - g_loss: 2496.2527\n",
            "Epoch 1358/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 28.5302 - g_loss: 2816.1561\n",
            "Epoch 1359/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -29.7950 - g_loss: 2497.9279\n",
            "Epoch 1360/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -3.2784 - g_loss: 2261.9046\n",
            "Epoch 1361/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -90.4854 - g_loss: 2219.6429\n",
            "Epoch 1362/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -61.6429 - g_loss: 1609.5806\n",
            "Epoch 1363/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -16.8187 - g_loss: 2267.9151\n",
            "Epoch 1364/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 68.7520 - g_loss: 2229.5839\n",
            "Epoch 1365/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: 21.2330 - g_loss: 2151.8274\n",
            "Epoch 1366/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -48.8755 - g_loss: 1735.6172\n",
            "Epoch 1367/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -30.4022 - g_loss: 1388.7157\n",
            "Epoch 1368/10000\n",
            "9/9 [==============================] - 22s 2s/step - d_loss: -9.0686 - g_loss: 1563.1291\n",
            "Epoch 1369/10000\n",
            "1/9 [==>...........................] - ETA: 0s - d_loss: 65.1891 - g_loss: 1781.5533"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
